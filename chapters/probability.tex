\section{Probability}

\mode<presentation>{
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability}
\tableofcontents[sectionstyle=show/hide,hideothersubsections]
\end{frame}
}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Introduction}
Descriptive Statistics provides methods to describe the variables measured in the sample and their relations, but it does not allow to draw any conclusion about the population.

Now it is time to take the leap from the sample to the population and the bridge for that is \highlight{probability theory}.

Remember that the sample has a limited information about the population, and in order to draw valid conclusions for the population the sample must be representative of it.
For that reason, to guarantee the representativeness of the sample, this must be drawn randomly. 
This means that the choice of individuals in the sample is by chance. 

Probability theory will provide us the tools to control the random in the sampling and to determine the level of reliability of the conclusions drawn from the sample. 
\end{frame}


\subsection{Random experiments and events}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Random experiments}
The study of a characteristic of the population is conducted through random experiments. 

\begin{definition}[Random experiment] A \emph{random experiment} is an experiment that meets two conditions:
\begin{enumerate}
\item The set of possible outcomes is known. 
\item It is impossible to predict the outcome with absolute certainty.
\end{enumerate} 
\end{definition}

\textbf{Example}. Gambling are typical examples of random experiments. 
The roll of a dice, for example, is a random experiment because
\begin{enumerate}
\item It is known the set of possible outcomes: $\{1,2,3,4,5,6\}$.
\item Before rolling the dice, it is impossible to predict with absolute certainty the outcome. 
\end{enumerate}

Another non-gambling example is the random choice of an individual of a human population and the determination of its blood type. 

Generally, the draw of a sample by a random method  is an random experiment.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sample space}
\begin{definition}[Sample space]
The set $\Omega$ of the possible outcomes of a random experiment is known as the \emph{sample space}.
\end{definition}

\textbf{Example} Some examples of sample spaces are:
\begin{itemize}
\item For the toss of a coin $\Omega=\{heads,tails\}$.
\item For the roll of a dice $\Omega=\{1,2,3,4,5,6\}$.
\item For the blood type of an individual drawn by chance $\Omega=\{\mbox{A},\mbox{B},\mbox{AB},\mbox{0}\}$.
\item For the height of an individual drawn by chance $\Omega=\mathbb{R}^+$.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Tree diagrams}
In experiments where more than one variable is measured, the determination of the sample space can be difficult. 
In such a cases, it is advisable to use a \highlight{tree diagram} to construct the sample space. 

In a tree diagram every variable is represented in a level of the tree and every possible outcome of the variable as a
branch.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Tree diagram}
\framesubtitle{Example of gender and blood type}
% The tree below represents the sample space of a random experiment where we measure the gender and the blood type of a
% person.

\begin{center}
\tikzsetnextfilename{probability/sample_space}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/sample_space}}}
\mode<presentation>{\resizebox{0.8\textwidth}{!}{\input{img/probability/sample_space}}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Random events}
\begin{definition}[Random event]
A \emph{random event} is any subset of the sample space $\Omega$ of a random experiment.
\end{definition}

There are different types of events:
\begin{itemize}
\item \textbf{Impossible event:} Is the event with no elements $\emptyset$. It has no chance of occurring.
\item \textbf{Elemental events:} Are events with only one element, that is, a singleton.
\item \textbf{Composed events:} Are events with two or more elements.
\item \textbf{Sure event:} Is the event that contains the whole sample space $\Omega$. It always happens.
\end{itemize}
\end{frame}


\subsection{Set theory}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Event space}
\begin{definition}[Event space] Given a sample space $\Omega$ of a random experiment, the \emph{event space} of
$\Omega$ is the set of all possible events of $\Omega$, and is noted $\mathcal{P}(\Omega)$.
\end{definition}

\textbf{Example}. Given the sample space $\Omega=\{a,b,c\}$, its even space is 
\[
\mathcal{P}(\Omega)=\left\{\emptyset, \{a\},\{b\},\{c\},\{a,b\},\{a,c\},\{b,c\},\{a,b,c\}\right\}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Event operations}
As events are subsets of the sample space, using the set theory we have the following operations on events:
\begin{itemize}
\item Union
\item Intersection
\item Complement
\item Difference
\end{itemize}
\end{frame}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Union of events}
\begin{definition}[Union event]
Given two events $A,B\subseteq \Omega$, the \emph{union} of $A$ and $B$, denoted by $A\cup B$, is the event of
all elements that are members of $A$ or $B$ or both.
\[
A\cup B = \{x\,|\, x\in A\textrm{ or }x\in B\}.
\]
\end{definition}

\begin{center}
\tikzsetnextfilename{probability/union}
\input{img/probability/union}
\end{center}
The union event $A\cup B$ happens when $A$ \alert{or} $B$ happen.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Intersection of events}
\begin{definition}[Intersection event]
Given two events $A,B\subseteq \Omega$, the \emph{intersection} of $A$ and $B$, denoted by $A\cap B$, is the
event of all elements that are members of both $A$ and $B$.
\[
A\cap B = \{x\,|\, x\in A\mbox{ and }x\in B\}.
\]
\end{definition}

\begin{center}
\tikzsetnextfilename{probability/intersection}
\input{img/probability/intersection}
\end{center}
The intersection event $A\cap B$ happens when $A$ \alert{and} $B$ happen.

% Observe that the intersection event is included in the union even $A\cap B \subseteq A\cup B$.

Two events are \highlight{incompatible} if their intersection is empty.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Complement of an event}
\begin{definition}[Complementary event]
Given an event $A\subseteq \Omega$, the \emph{complementary or contrary event} of $A$, denoted by $\overline A$, is
the event of all elements of $\Omega$ except the elements that are members of $A$.
\[
\overline A = \{x\,|\, x\not\in A\}.
\]
\end{definition}

\begin{center}
\tikzsetnextfilename{probability/complement}
\input{img/probability/complement}
\end{center}

The complementary event $\overline A$ happens when $A$ does \alert{not} happen.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Difference of events}
\begin{definition}[Difference event]
Given two events $A,B\subseteq \Omega$, the \emph{difference} of $A$ and $B$, denoted by $A-B$, is the
event of all elements that are members of $A$ but not are members of $B$.
\[
A-B = \{x\,|\, x\in A\mbox{ and }x\not\in B\} = A \cap \overline B.
\]
\end{definition}

\begin{center}
\tikzsetnextfilename{probability/difference}
\input{img/probability/difference}
\end{center}

The difference event $A-B$ happens when $A$ happens but $B$ does not.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Event operations}
\framesubtitle{Example}
Given the sample space of rolling a dice $\Omega=\{1,2,3,4,5,6\}$ and the events $A=\{2,4,6\}$ and $B=\{1,2,3,4\}$, 
\begin{itemize}
\item The union of $A$ and $B$ is $A\cup B=\{1,2,3,4,6\}$.
\item The intersection of $A$ and $B$ is $A\cap B=\{2,4\}$.
\item The complement of $A$ is $\overline A=\{1,3,5\}$.
\item The events $A$ and $\overline A$ are incompatible.
\item The difference of $A$ and $B$ is $A-B=\{6\}$, and the difference of $B$ and $A$ is $B-A=\{1,3\}$.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Algebra of events}
Given the events $A,B,C\subseteq \Omega$, the following properties are meet: 
\begin{enumerate}[<+->]
\item $A\cup A=A$,\quad $A\cap A=A$ (idempotence).
\item $A\cup B=B\cup A$,\quad $A\cap B = B\cap A$ (commutative).
\item $(A\cup B)\cup C = A\cup (B\cup C)$,\quad $(A\cap B)\cap C = A\cap (B\cap C)$ (associative).
\item $(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$,\quad $(A\cap B)\cup C = (A\cup C)\cap (B\cup C)$ (distributive).
\item $A\cup \emptyset=A$,\quad $A\cap \Omega=A$ (neutral element).
\item $A\cup \Omega=\Omega$,\quad $A\cap \emptyset=\emptyset$ (absorbing element).
\item $A\cup \overline A = \Omega$,\quad $A\cap \overline A= \emptyset$ (complementary symmetric element).
\item $\overline{\overline A} = A$ (double contrary).
\item $\overline{A\cup B} = \overline A\cap \overline B$,\quad $\overline{A\cap B} = \overline A\cup \overline B$ (Morgan's
laws).
\end{enumerate}
\end{frame}


\subsection{Probability definition}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Classical definition of probability}
\begin{definition}[Probability --- Laplace]
Given a sample space $\Omega$ of a random experiment where all elements of $\Omega$ are equally likely, the
\emph{probability} of an event $A\subseteq \Omega$ is the quotient between the number of elements of $A$ and the number
of elements of $\Omega$
\[ P(A) = \frac{|A|}{|\Omega|} = \frac{\mbox{number of favorable outcomes}}{\mbox{number of possible outcomes}}\]
\end{definition}

This definition is well known, but it has important restrictions:
\begin{itemize}
\item It is required that all the elements of the sample space are equally likely (\emph{equiprobability}).
\item It can not be used with infinite sample spaces.
\end{itemize}

\alert{\emph{Watch out! These conditions are not meet in many real experiments.}}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Classical definition of probability}
\framesubtitle{Example}
Given the sample space of rolling a dice $\Omega=\{1,2,3,4,5,6\}$ and the event $A=\{2,4,6\}$, the probability of $A$ is 
\[
P(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = 0.5.
\]

However, given the sample space of the blood type of a random individual $\Omega=\{O,A,B,AB\}$, it is not possible to use
the classical definition to compute the probability of having group $A$,
\[
P(A) \neq \frac{|A|}{|\Omega|} = \frac{1}{4} = 0.25,
\]
because the blood types are not equally likely in human populations. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency definition of probability}
\begin{theorem}[Law of large numbers]
When a random experiment is repeated a large number of times, the relative frequency of an event tends to the probability of the event.
\end{theorem}

The following definition of probability uses this theorem.
\begin{definition}[Frequency probability]
Given a sample space $\Omega$ of a replicable random experiment, the \emph{probability} of an event $A\subseteq \Omega$ is the relative frequency of the event $A$ in an infinite number of repetitions of the experiment 
\[
P(A) = lim_{n\rightarrow \infty}\frac{n_A}{n}
\]
\end{definition}

This definition also have some drawbacks
\begin{itemize}
\item It computes an estimation of the real probability. % (more accurate the higher the sample size).
\item The repetition of the experiment must be in identical conditions.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency definition of probability}
\framesubtitle{Example}
Given the sample space of tossing a coin $\Omega=\{H,T\}$, if after tossing the coin 100 times we got 54 heads, then the probability of $H$ is
\[
P(H) = \frac{n_H}{n} = \frac{54}{100} = 0.54.
\]

Given the sample space of the blood type of a random individual $\Omega=\{O,A,B,AB\}$, if after drawing a random sample of 1000 persons we got 412 with blood type $A$, then the probability of $A$ is 
\[
P(A) = \frac{n_A}{n} = \frac{412}{1000} = 0.412.
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Axiomatic definition of probability}
\begin{definition}[Probability --- Kolmog√≥rov]
Given a sample space $\Omega$ of a random experiment, a \emph{probability} function is a function that maps
every event $A\subseteq \Omega$ a real number $P(A)$, known as the probability of $A$, that meets the following axioms:
\begin{enumerate}
\item The probability of any event is nonnegative, 
\[
P(A)\geq 0.
\]
\item The probability of the sure event is 1,
\[
P(\Omega)=1
\] 
\item The probability of the union of two incompatible events ($A\cap B=\emptyset$) is the sum of their probabilities
\[P(A\cup B) = P(A)+P(B).\]
\end{enumerate}
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Properties of the axiomatic probability}
From the previous axioms is possible to deduce some important properties of a probability function. 

Given a sample space $\Omega$ of a random experiment and the events $A,B\subseteq \Omega$, the following properties are
meet:
\begin{enumerate}
\item <2-> $P(\overline A) = 1-P(A)$.
\item <3-> $P(\emptyset)= 0$.
\item <4-> If $A\subseteq B$ then $P(A)\leq P(B)$.
\item <5-> $P(A) \leq 1$. This means that $P(A)\in [0,1]$.
\item <6-> $P(A-B)=P(A)-P(A\cap B)$. 
\item <7-> $P(A\cup B)= P(A) + P(B) - P(A\cap B)$.
\item <8-> If $A=\{e_1,\ldots,e_n\}$, where $e_i$ $i=1,\ldots,n$ are elemental events, then
\[
P(A)=\sum_{i=1}^n P(e_i).
\]
\end{enumerate}

\mode<article>{
\textbf{Proof.}
\begin{enumerate}
\item $\overline A = \Omega \Rightarrow P(A\cup \overline A) = P(\Omega) \Rightarrow P(A)+P(\overline A) = 1 \Rightarrow
P(\overline A)=1-P(A)$.
\item $\emptyset = \overline \Omega \Rightarrow P(\emptyset) = P(\overline \Omega) = 1-P(\Omega) = 1-1 = 0.$
\item $B = A\cup (B-A)$. As $A$ and $B-A$ are incompatible, $P(B) = P(A\cup (B-A)) = P(A)+P(B-A) \geq
P(A).$

If we think of probabilities as areas, it is easy to see graphically,
\begin{center}
\tikzsetnextfilename{probability/inclusion_probability}
\input{img/probability/inclusion_probability}
\end{center}

\item $A\subseteq \Omega \Rightarrow P(A)\leq P(\Omega)=1.$
\item $A=(A-B)\cup (A\cap B)$. As $A-B$ and $A\cap B$ are incompatible, $P(A)=P(A-B)+P(A\cap B) \Rightarrow 
P(A-B)=P(A)-P(A\cap B)$.

If we think of probabilities as areas, it is easy to see graphically, 
\begin{center}
\tikzsetnextfilename{probability/difference_probability}
\input{img/probability/difference_probability}
\end{center}

\item $A\cup B= (A-B) \cup (B-A) \cup (A\cap B)$. As $A-B$, $B-A$ and $A\cap B$ are incompatible, $P(A\cup
B)=P(A-B)+P(B-A)+P(A\cap B) = P(A)-P(A\cap B)+P(B)-P(A\cap B)+P(A\cap B)= P(A)+P(B)-P(A\cup B)$.

If we think again of probabilities as areas, it is easy to see graphically because the area of $A\cap B$ is added twice (one for $A$ and other for $B$), so it must be subtracted once. 
\begin{center}
\tikzsetnextfilename{probability/union_probability}
\input{img/probability/union_probability}
\end{center}
\item $A=\{e_1,\cdots,e_n\} = \{e_1\}\cup \cdots \cup \{e_n\} \Rightarrow P(A)=P(\{e_1\}\cup \cdots \cup \{e_n\}) =
P(\{e_1\})+ \cdots P(\{e_n\}).$
\end{enumerate}
}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability interpretation}
As set by the previous axioms, the probability of an event $A$, is a real number $P(A)$ that always ranges from 0 to 1. 

In a certain way, this number expresses the plausibility of the event, that is, the chances that the event $A$ occurs in the experiment.
Therefore, it also gives a measure of the uncertainty about the event.
\begin{itemize}
\item The maximum uncertainty correspond to probability $P(A)=0.5$ ($A$ and $\overline A$ have the same chances of happening).
\item The minimum uncertainty correspond to probability $P(A)=1$ ($A$ will happen with absolute certainty) and $P(A)=0$ ($A$ won't happen with absolute certainty).
\end{itemize} 

When $P(A)$ is closer to 0 than to 1, the chances of not happening $A$ are greater than the chances of happening $A$.
On the contrary, when $P(A)$ is closer to 1 than to 0, the chances of happening $A$ are greater than the chances of not happening $A$.
\end{frame}


\subsection{Conditional probability}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Conditional experiments}
Occasionally, we can get some information about the experiment before its realization. 
Usually that information is given as an event $B$ of the same sample space that we know that is true before we conduct the experiment.

In such a case, we will say that $B$ is a \emph{conditioning} event and the probability of another event $A$ is known as a \highlight{conditional probability} and expressed 
\[
P(A|B).
\]

This must be read as \emph{probability of $A$ given $B$} or \emph{probability of $A$ under the condition $B$}.
\end{frame}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Conditional experiments}
\framesubtitle{Example}
Usually, conditioning events change the sample space and therefore the probabilities of events.
 
Assume that we have a sample of 100 women and 100 men with the following frequencies
\[
\begin{array}{|c|c|c|}
\cline{2-3}
 \multicolumn{1}{c|}{} & \mbox{Non-smokers} & \mbox{Smokers} \\ \hline
 \rowcolor{color1!30} \mbox{Females} & 80 & 20 \\ \hline
 \mbox{Males} & 60 & 40 \\ \hline
\end{array}
\]
Then, using the frequency definition of probability, the probability of being smoker from the whole sample is
\[
P(\mbox{Smoker})= \frac{60}{200}=0.3.
\]

\pause

However, if we know that the person is a woman, then the sample is reduced to the first row, and the probability of being smoker is 
\[
P(\mbox{Smoker}|\mbox{Female})=\frac{20}{100}=0.2.
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Conditional probability}
\begin{definition}[Conditional probability]
Given a sample space $\Omega$ of a random experiment, and two events $A,B\subseteq \Omega$, the probability of $A$ \emph{conditional} on $B$ occurring is
\[
P(A|B) = \frac{P(A\cap B)}{P(B)},
\]
as long as, $P(B)\neq 0$.
\end{definition}

This definition allows to calculate conditional probabilities without changing the original sample space. 

\textbf{Example}. In the previous example
\[
P(\mbox{Smoker}|\mbox{Female})= \frac{P(\mbox{Smoker}\cap \mbox{Female})}{P(\mbox{Female})} = \frac{20/200}{100/200}=\frac{20}{100}=0.2.
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability of the intersection event}
From the definition of conditional probability it is possible to derive the formula for the probability of the intersection of two events. 
\[
P(A\cap B) = P(A)P(B|A) = P(B)P(A|B).
\]

\textbf{Example}. In a population there are a 30\% of smokers and we know that there are a 40\% of smokers with breast cancer. 
The probability of a random person being smoker and having breast cancer is 
\[
P(\mbox{Smoker}\cap \mbox{Cancer})= P(\mbox{Smoker})P(\mbox{Cancer}|\mbox{Smoker}) =
0.3\times 0.4 = 0.12.
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Independence of events}
Sometimes, the conditioning event does not change the original probability of the main event. 
\begin{definition}[Independent events]
Given a sample space $\Omega$ of a random experiment, two events $A,B\subseteq \Omega$ are \emph{independents} if the probability of $A$ does not change when conditioning on $B$, and vice-versa, that is,
\[
P(A|B) = P(A) \quad \mbox{and} \quad P(B|A)=P(B),
\]
if $P(A)\neq 0$ and $P(B)\neq 0$.
\end{definition}

This means that the occurrence of one event does not give relevant information to change the uncertainty of the other.

When two events are independent, the probability of the intersection of them is equal to the product of their probabilities,
\[
P(A\cap B) = P(A)P(B).
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Independence of events}
\framesubtitle{Example of tossing coins}
The sample space of tossing twice a coin is $\Omega=\{(H,H),(H,T),(T,H),(T,T)\}$ and all the elements are equiprobable if the coin is fair. 
Thus, applying the classical definition of probability we have 
\[
P((H,H)) = \frac{1}{4} = 0.25.
\]

If we name $H_1=\{(H,H),(H,T)\}$, that is, having heads in the first toss, and $H_2=\{(H,H),(T,H)\}$, that is, having
heads in the second toss, we can get the same result assuming that these events are independent,
\[
P(H,H)= P(H_1\cap H_2) = P(H_1)P(H_2) = \frac{2}{4}\frac{2}{4}=\frac{1}{4}=0.25.
\] 
\end{frame}


\subsection{Probability space}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability space}
\begin{definition}[Probability space]
A \emph{probability space} of a random experiment is a triplet $(\Omega,\mathcal{F},P)$ where
\begin{itemize}
\item $\Omega$ is the sample space of the experiment.
\item $\mathcal{F}$ is a set of events of the experiment.
\item $P$ is a probability function. 
\end{itemize} 
\end{definition}

If we know the probabilities of all the elements of $\Omega$, then we can calculate the probability of every event in $\mathcal{F}$ and we can construct easily the probability space. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability space construction}
In order to determine the probability of every elemental event we can use a tree diagram, using the following rules:
\begin{enumerate}
\item For every node of the tree, label the incoming edge with the probability of the variable in that level having the value of the node, conditioned by events corresponding to its ancestor nodes in the tree.
\item The probability of every elemental event in the leaves is the product of the probabilities on edges
that go form the root to the leave.
\end{enumerate}
\begin{center}
\tikzsetnextfilename{probability/probability_space}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/probability/probability_space}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/probability/probability_space}}}
\end{center}
\end{frame}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability tree with dependent variables}
\framesubtitle{Example of smoking and cancer}
In a population there are a 30\% of smokers and we know that there are a 40\% of smokers with breast cancer, while only 10\% of non-smokers have breast cancer. 
The probability tree of the probability space of the random experiment consisting of picking a random person and
measuring the variables smoking and breast cancer is shown below.  
\begin{center}
\tikzsetnextfilename{probability/smoking_cancer_probability_space}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/smoking_cancer_probability_space}}}
\mode<presentation>{\resizebox{0.8\textwidth}{!}{\input{img/probability/smoking_cancer_probability_space}}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability tree with independent variables}
\framesubtitle{Example of tossing coins}
The probability tree of the random experiment of tossing two coins is shown below.
\begin{center}
\tikzsetnextfilename{probability/coins_probability_space}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/coins_probability_space}}}
\mode<presentation>{\resizebox{0.8\textwidth}{!}{\input{img/probability/coins_probability_space}}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Probability tree with independent variables}
\framesubtitle{Example of a sample of size 3}
In a population there are 40\% of males and 60\% of females, the probability tree of drawing a random sample of three persons is shown below.
\begin{center}
\tikzsetnextfilename{probability/sample_probability_space}
\mode<article>{\resizebox{0.7\textwidth}{!}{\input{img/probability/sample_probability_space}}}
\mode<presentation>{\resizebox{0.9\textwidth}{!}{\input{img/probability/sample_probability_space}}}
\end{center}
\end{frame}


\subsection{Total probability theorem}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Partition of the sample space}
\begin{definition}[Partition of the sample space]
A collection of events $A_1,A_2,\ldots,A_n$ of the same sample space $\Omega$ is a \emph{partition} of the sample space if it satisfies the following conditions
\begin{enumerate}
\item The union of the events is the sample space, that is, $A_1\cup \cdots\cup A_n =\Omega$.
\item All the events are mutually incompatible, that is, $A_i\cap A_j = \emptyset$ $\forall i\neq j$.
\end{enumerate}
\end{definition}

\begin{center}
\tikzsetnextfilename{probability/sample_space_partition}
\input{img/probability/sample_space_partition}
\end{center}

Usually it is easy to get a partition of the sample space splitting a population according to some categorical variable, like for example gender, blood type, etc.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Total probability theorem}
If we have a partition of a sample space, we can use it to calculate the probabilities of other events in the same sample space.
\begin{theorem}[Total probability]
Given a partition $A_1,\ldots,A_n$ of a sample space $\Omega$, the probability of any other event $B$ of the same sample
space can be calculated with the formula
\[
P(B) = \sum_{i=1}^n P(A_i\cap B) = \sum_{i=1}^n P(A_i)P(B|A_i).
\]
\end{theorem}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Total probability theorem}
\framesubtitle{Proof}
The proof of the theorem is quite simple.
As $A_1,\ldots,A_n$ is a partition of $\Omega$, we have 
\[
B = B\cap \Omega = B\cap (A_1\cup \cdots \cup A_n) = (B\cap A_1)\cup \cdots \cup (B\cap A_n).
\]
And all the events of this union are mutually incompatible as $A_1,\ldots,A_n$ are, thus
\begin{align*}
P(B) &= P((B\cap A_1)\cup \cdots \cup (B\cap A_n)) = P(B\cap A_1)+\cdots + P(B\cap A_n) =\\
&= P(A_1)P(B|A_1)+\cdots + P(A_n)P(B|A_n) = \sum_{i=1}^n P(A_i)P(B|A_i).
\end{align*}

\begin{center}
\tikzsetnextfilename{probability/total_probability}
\mode<article>{\resizebox{0.4\textwidth}{!}{\input{img/probability/total_probability}}}
\mode<presentation>{\input{img/probability/total_probability}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Total probability theorem}
\framesubtitle{Example of diagnosis}
A symptom $S$ can be caused by a disease $D$, but it can also be present in persons without the disease.
In a population, the rate of people with the disease is $0.2$. 
We know also that $90\%$ of persons with the disease have the symptom, while only $40\%$ of persons without the disease have it. 

\emph{What is the probability that a random person of the population has the symptom?}

To answer the question we can apply the total probability theorem using the partition $\{D,\overline D\}$:
\[
P(S) = P(D)P(S|D)+P(\overline D)P(S|\overline D) = 0.2\cdot 0.9 + 0.8\cdot 0.4 = 0.5.
\]
That is, half of the population has the symptom. 

\begin{center}
\emph{Indeed, it is a weighted mean of probabilities!}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Total probability theorem}
\framesubtitle{Example of diagnosis with a tree diagram}
The answer to the previous question is even clearer with the tree diagram of the probability space.

\begin{center}
\tikzsetnextfilename{probability/total_probability_space}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/total_probability_space}}}
\mode<presentation>{\resizebox{0.8\textwidth}{!}{\input{img/probability/total_probability_space}}}
\end{center}

\begin{align*}
P(S) &= P(D\cap S) + P(\overline D\cap S) = P(D)P(S|D)+P(\overline D)P(S|\overline D)\\
& = 0.2\cdot 0.9+ 0.8\cdot 0.4 = 0.18 + 0.32 = 0.5.
\end{align*}
\end{frame}


\subsection{Bayes theorem}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Bayes theorem}
A partition of a sample space $A_1,\cdots,A_n$ may also be interpreted as a set of feasible hypothesis for a fact $B$.

In such cases it may be helpful to calculate the posterior probability $P(A_i|B)$ of every hypothesis.

\begin{theorem}[Bayes]
Given a partition $A_1,\ldots,A_n$ of a sample space $\Omega$ and another event $B$ of the same sample space, the
conditional probability of every even $A_i$ $i=1,\ldots,n$ on $B$ can be calculated with the following formula
\[
P(A_i|B) = \frac{P(A_i\cap B)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{i=1}^n P(A_i)P(B|A_i)}.
\]
\end{theorem}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Bayes theorem}
\framesubtitle{Example of diagnosis}
In the previous example, a more interesting question is about the diagnosis for a person with the symptom.  

In this case we can interpret $D$ and $\overline{D}$ as the two feasible hypothesis for the symptom $S$.
The prior probabilities for them are $P(D)=0.2$ and $P(\overline{D})=0.8$.
That means that if we do not have information about the symptom, the diagnosis would be that the person does not have the disease.

However, if after examining the person we observe the symptom, that information changes the uncertainty about the hypothesis, and we need calculate the posterior probabilities to diagnose, that is,
\[
P(D|S) \mbox{ and } P(\overline{D}|S)
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Bayes theorem}
\framesubtitle{Example of diagnosis}
To calculate the posterior probabilities we can use the Bayes theorem.
\begin{align*}
P(D|S) &= \frac{P(D)P(S|D)}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.2\cdot 0.9}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.18}{0.5}=0.36,\\
P(\overline{D}|S) &= \frac{P(\overline{D})P(S|\overline{D})}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.8\cdot 0.4}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.32}{0.5}=0.64.
\end{align*}

As we can see the probability of having the disease has increased. 
Nevertheless, the probability of not having the disease is still greater than the probability of having it, and for that
reason, the diagnosis is not having the disease. 

In this case it is said the the symptom $S$ is \emph{not decisive} in order to diagnose the disease.
\end{frame}


\subsection{Epidemiology}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Epidemiology}
One of the branches of Medicine that makes an intensive use of probability is \highlight{Epidemilogy}, that study the distribution and causes of diseases in populations identifying risk factors for disease and targets for preventive healthcare.

In Epidemiology we are interested in how often appears a \emph{medical event} $D$ (typically a disease like flu, a risk factor like smoking or a protection factor like a vaccine) that is measured as a nominal variable with two categories (occurrence or not of the event). 

There are different measures related to the frequency of a medical event.
The most important are:
\begin{itemize}
  \item Prevalence
  \item Incidence
  \item Relative risk
  \item Odds ratio
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Prevalence}
\begin{definition}[Prevalence]
The \emph{prevalence} of a medical event $D$ is the proportion of a particular population that is affected by a medical event.
\[
  \mbox{Prevalence}(D) = \frac{\mbox{Num people affected by $D$}}{\mbox{Population size}}
\]
\end{definition}

Often, the prevalence is estimated from a sample as the relative frequency of people affected by the event in the sample.
It is also common to express that frequency as a percentage. 

\textbf{Example}. To estimate the prevalence of flu a sample of 1000 persons has been studied and 150 of them had flu. 
Thus, the prevalence of flu is approximately 150/1000=0.15, that is, a 15\%. 

\end{frame}
  

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Incidence}
\highlight{Incidence} measures the likelihood of occurrence of a medical event in a population within a given period of time.
Incidence can be measured as a cumulative proportion or as a rate.

\begin{definition}[Cumulative incidence]
The \emph{cumulative incidence} of a medical event $D$ is the proportion of people that acquired the event in a period of time, that is, the number of new cases with the event in the period of time divided by the size of the population at risk.
\[
  R(D)=\frac{\mbox{Num of new cases with $D$}}{\mbox{Population at risk size}}
\]
\end{definition}

\textbf{Example}. A population initially contains $1000$ persons without flu and after two years of observation 160 of them got the flu. 
The incidence proportion of flu is 160 cases per $1000$ persons per two years, i.e. 16\% per two years.
\end{frame}
  

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Incidence rate or Absolute risk}
  
\begin{definition}[Incidence rate]
The \emph{incidence rate} or \emph{absolute risk} of a medical event $D$ is the number of new cases with the event divided by the size of the population at risk and by the number of units of time in a given period.
\[
  R(D)=\frac{\mbox{Num of new cases with $D$}}{\mbox{Population at risk size}\times \mbox{Num of time units}}
\]
\end{definition}

\textbf{Example}. A population initially contains $1000$ persons without flu and after two years of observation 160 of them got the flu. 
If we consider the year as the unit of time, the incidence rate of flu is 160 cases per $1000$ persons divided by two years, i.e. 80 cases per 1000 persons-year or 8\% persons per year.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Prevalence vs Incidence}
Prevalence must not be confused with incidence. Prevalence indicates how widespread the medical event is, and is more a measure of the burden of the event on society with no regard to time at risk or when subjects may have been exposed to a possible risk factor, whereas incidence conveys information about the risk of being affected by the event.

Prevalence can be measured in cross-sectional studies at a particular time, while in order to measure incidence we need a longitudinal study observing the individuals during a period of time.

Incidence is usually more useful than prevalence in understanding the event etiology: for example, if the incidence of a disease in a population increases, then there is a risk factor that promotes it.

When the incidence is approximately constant for the duration of the event, prevalence is approximately the product of event incidence and average event duration, so 
\begin{center}
  prevalence = incidence $\times$ duration
\end{center}
\end{frame}
  

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Comparing risks}
In order to determine if a factor or characteristic is associated with the medical event we need to compare the risk of the medical event in two populations, one exposed to the factor and the other not exposed.
The group of people exposed to the factor is known as the \emph{treatment group} or the \emph{experimental group} and the group of people unexposed as the \emph{control group}.

Usually the cases observed for each group are represented in a 2$\times$2 table like the one below. 

\begin{center}
  \begin{tabular}{|m{3cm}|m{3cm}<{\centering}|m{3cm}<{\centering}|}
  \cline{2-3}
  \multicolumn{1}{c|}{} & Event $D$ & No event $\overline D$\\ 
  \hline
  Treatment group\newline (exposed) & $a$ & $b$\\ 
  \hline 
  Control group\newline (unexposed) & $c$ & $d$\\ 
  \hline
  \end{tabular}
\end{center}
\end{frame}
  

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Attributable risk or Risk difference $RD$}
\begin{definition}[Attributable risk]
The \emph{attributable risk} or \emph{risk difference} of a medical event $D$ for people exposed to a factor is the difference between the absolute risks of the treatment group and the control group.
\[
  AR(D)=R_T(D)-R_C(D)=\frac{a}{a+b}-\frac{c}{c+d}.
\]
\end{definition}

The attributable risk is the risk of an event that is specifically due to the factor of interest.

Observe that the attributable risk can be positive, when the risk of the treatment group is greater than the risk of the control group, and negative, on the contrary.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
  \frametitle{Attributable risk $RR$}
  \framesubtitle{Example of a vaccine}
  To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
  Half of them were vaccinated (treatment group) and the other received a placebo (control group).
  The table below summarize the results at the end of the year. 
  
  \begin{center}
    \begin{tabular}{|m{2.7cm}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|}
    \cline{2-3}
    \multicolumn{1}{c|}{} & Flu $D$ & No flu $\overline D$\\ 
    \hline
    Treatment group\newline (vaccinated) & $20$ & $480$\\ 
    \hline 
    Control group\newline (Unvaccinated) & $80$ & $420$\\ 
    \hline
  \end{tabular}
  \end{center}
  
  The attributable risk of getting the flu for people vaccinated is
  \[
    AR(D) = \frac{20}{20+480}-\frac{80}{80+420} = -0.12.
  \]
  This means that the risk of getting flu in vaccinated people is a 12\% less than in unvaccinated. 
  \end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Relative risk $RR$}
\begin{definition}[Relative risk]
The \emph{relative risk} of a medical event $D$ for people exposed to a factor is the quotient between the proportions of people that acquired the event in a period of time in the treatment and control groups.
That is, the quotient between the incidences of the treatment and the control groups.
\[
  RR(D)=\frac{\mbox{Risk in treatment group}}{\mbox{Risk in control group}}=\frac{R_T(D)}{R_C(D)}=\frac{a/(a+b)}{c/(c+d)}
\]
\end{definition}

Relative risk compares the risk of a medical event between the treatment and the control groups. 
\begin{itemize}
  \item $RR=1$ $\Rightarrow$ There is no association between the event and the exposure to the factor. 
  \item $RR<1$ $\Rightarrow$ Exposure to the factor decreases the risk of the event.
  \item $RR>1$ $\Rightarrow$ Exposure to the factor increases the risk of the event.
\end{itemize}
The further from 1, the stronger the association. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Relative risk $RR$}
\framesubtitle{Example of a vaccine}
To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
Half of them were vaccinated (treatment group) and the other received a placebo (control group).
The table below summarize the results at the end of the year. 

\begin{center}
  \begin{tabular}{|m{2.7cm}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|}
  \cline{2-3}
  \multicolumn{1}{c|}{} & Flu $D$ & No flu $\overline D$\\ 
  \hline
  Treatment group\newline (vaccinated) & $20$ & $480$\\ 
  \hline 
  Control group\newline (Unvaccinated) & $80$ & $420$\\ 
  \hline
\end{tabular}
\end{center}

The relative risk of getting the flu for people vaccinated is
\[
  RR(D) = \frac{20/(20+480)}{80/(80+420)} = 0.25.
\]
This means that vaccinated people were only one-fourth as likely to develop flu as were unvaccinated people, i.e. the vaccine reduce the risk of flu by 75\%.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Odds}
An alternative way of measuring the risk of a medical event is the \emph{odds}.

\begin{definition}[Odds]
The \emph{odds} of a medical event $D$ in a population is the quotient between the people that acquired the event and people that not in a period of time.
\[
	ODDS(D)=\frac{\mbox{Num new cases with $D$}}{\mbox{Num cases without $D$}}=\frac{P(D)}{P(\overline D)}
\]
\end{definition}

Unlike incidence, that is a proportion less than or equal to 1, the odds can be greater than 1. 
However, it is possible to convert an odds into a probability with the formula
\[
  P(D) = \frac{ODDS(D)}{ODDS(D)+1}
\]

\textbf{Example} A population initially contains $1000$ persons without flu and after a year 160 of them got the flu. 
The odds of flu is 160/840.

Observe that the incidence is 160/1000. 
\end{frame}
 

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Odds ratio $OR$}
\begin{definition}[Odds ratio]
The \emph{odds ratio} of a medical event $D$ for people exposed to a factor is the quotient between the odds of the event of the treatment and the control groups.
\[
  OR(D)=\frac{\mbox{Odds in treatment group}}{\mbox{Odds in control group}}=\frac{a/b}{c/d}=\frac{ad}{bc}
\]
\end{definition}

Odds ratio compares the odds of a medical event between the treatment and the control groups. 
The interpretation is similar to the relative risk. 
\begin{itemize}
  \item $OR=1$ $\Rightarrow$ There is no association between the event and the exposure to the factor. 
  \item $OR<1$ $\Rightarrow$ Exposure to the factor decreases the risk of the event.
  \item $OR>1$ $\Rightarrow$ Exposure to the factor increases the risk of the event.
\end{itemize}
The further from 1, the stronger the association. 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Odds ratio $OR$}
\framesubtitle{Example of a vaccine}
To determine the effectiveness of a vaccine against the flu, a sample of 1000 person without flu was selected at the beginning of the year.
Half of them were vaccinated (treatment group) and the other received a placebo (control group).
The table below summarize the results at the end of the year. 

\begin{center}
  \begin{tabular}{|m{2.7cm}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|}
  \cline{2-3}
  \multicolumn{1}{c|}{} & Flu $D$ & No flu $\overline D$\\ 
  \hline
  Treatment group\newline (vaccinated) & $20$ & $480$\\ 
  \hline 
  Control group\newline (Unvaccinated) & $80$ & $420$\\ 
  \hline
\end{tabular}
\end{center}

The odds ratio of getting the flu for people vaccinated is
\[
  OR(D) = \frac{20/480}{80/420} = 0.21875.
\]
This means that the odds of getting the flu versus not getting the flu in vaccinated individuals is almost one fifth of that in unvaccinated, i.e. approximately for every 22 persons vaccinated with flu there will be 100 persons unvaccinated with flu. 
\end{frame}


\begin{frame}
\frametitle{Relative risk vs Odds ratio} 
Relative risk and odds ratio are two measures of association but their interpretation is slightly different.
While the relative risk expresses a comparison of risks between the treatment and control groups, the odds ratio expresses a comparison of odds, that is not the same than the risk.
Thus, an odds ratio of 2 \emph{does not} mean that the treatment group has the double of risk of acquire the medical event. 

The interpretation of the odds ratio is trickier because is counterfactual, and give us how many times is more frequent the event in the treatment group in comparison with the control group, assuming that in the control group the event is as frequent as the non-event.

The advantage of the odds ratio is that it does not depend on the prevalence or the incidence of the event, and must be used necessarily when the number of people with the medical event is selected arbitrarily in both groups, like in the case-control studies.
\end{frame}


\begin{frame}
\frametitle{Relative risk vs Odds ratio}
\framesubtitle{Example of lung cancer and smoking}
In order to determine the association between lung cancer and smoking two samples were selected (the second one with the double of non-cancer individuals) getting the following results:

\bigskip
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Sample 1}
\begin{center}
\small
\begin{tabular}{|m{1.9cm}|m{1.2cm}<{\centering}|m{1.5cm}<{\centering}|}
\cline{2-3}
\multicolumn{1}{c|}{} & Cancer & No cancer\\ 
\hline
Smokers & $60$ & $80$\\ 
\hline 
Non-smokers & $40$ & $320$\\ 
\hline
\end{tabular}
\end{center}

\begin{align*}
RR(D) &= \frac{60/(60+80)}{40/(40+320)} = 3.86.\\
OR(D) &= \frac{60/80}{40/320} = 6. 
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Sample 2}
\begin{center}
\small
\begin{tabular}{|m{1.9cm}|m{1.2cm}<{\centering}|m{1.5cm}<{\centering}|}
\cline{2-3}
\multicolumn{1}{c|}{} & Cancer & No cancer\\ 
\hline
Smokers & $60$ & $160$\\ 
\hline 
Non-smokers & $40$ & $640$\\ 
\hline
\end{tabular}
\end{center}

\begin{align*}
RR(D) &= \frac{60/(60+160)}{40/(40+640)} = 4.64.\\
OR(D) &= \frac{60/160}{40/640} = 6. 
\end{align*}
\end{column}
\end{columns}
Thus, when we change the incidence or the prevalence of the event (lung cancer) the relative risk changes, while the odds ratio not. 
\end{frame}


\begin{frame}
\frametitle{Relative risk vs Odds ratio} 
The relation between the relative risk and the odds ratio is given by the following formula

\[
  RR = \frac{OR}{1-R_C+R_C*OR}=OR*\frac{1-R_T}{1-R_C},
\]

where $R_C$ and $R_T$ are the prevalence or the incidence in control and treatment groups respectively.

The odds ratio always overestimate the relative risk when it is greater than 1 and underestimate it when it is less than 1. 
However, with rare medical events (with very small prevalence or incidence) the relative risk and the odds ratio are almost the same. 
\end{frame}


\begin{frame}
\frametitle{Relative risk vs Odds ratio} 
\begin{center}
\tikzsetnextfilename{probability/odds_ratio_vs_relative_risk}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/odds_ratio_vs_relative_risk}}}
\mode<presentation>{\resizebox{!}{\textheight}{\input{img/probability/odds_ratio_vs_relative_risk}}}
\end{center}
\end{frame}



\subsection{Diagnostic tests}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Diagnostic tests}
In Epidemiology it is common to use diagnostic test to diagnose diseases.

In general, diagnostic tests are not fully reliable and have some risk of misdiagnosis as it is represented in the table below.

\begin{center}
\begin{tabular}{|m{2.5cm}|m{3cm}<{\centering}|m{3cm}<{\centering}|}
\cline{2-3}
\multicolumn{1}{c|}{} & Presence of disease $D$ & Absence of disease $\overline D$\\ \hline
Test outcome\newline positive $+$ & \textcolor{green}{True Positive}\newline $TP$ & \textcolor{red}{False
Positive}\newline $FP$\\ \hline Test outcome\newline negative $-$ & \textcolor{red}{False Negative}\newline $FN$ &
\textcolor{green}{True Negative}\newline $TN$\\ \hline
\end{tabular}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sensitivity and specificity of a diagnostic test}
The performance of a diagnostic test depends on the following two probabilities.
\begin{definition}[Sensitivity]
The \emph{sensitivity} of a diagnostic test is the proportion of positive outcomes in persons with the disease 
\[
P(+|D)=\frac{TP}{TP+FN}
\]
\end{definition}

\begin{definition}[Specificity]
The \emph{specificity} of a diagnostic test is the proportion of negative outcomes in persons without the disease
\[
P(-|\overline{D})=\frac{TN}{TN+FP}
\]
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sensitivity and specificity interpretation}
Usually, there is a trade-off between sensitivity and specificity.  

A test with high sensitivity will detect the disease in most sick persons, but it will produce also more false positives than a less sensitive test. 
This way, a positive outcome in a test with high sensitivity is not useful for confirming the disease, but a negative outcome is useful for ruling out the disease, since it rarely give negative outcomes in sick people.

On the other hand, a test with a high specificity will rule out the disease in most healthy persons, but it will produce also more false negatives than a less specific test. 
Thus, a negative outcome in a test with high specificity is not useful for ruling out the disease, but a positive is useful to confirm the disease, since it rarely give positive outcomes in healthy people.  
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sensitivity and specificity interpretation}
Deciding on a test with greater sensitivity or a test with greater specificity depends on the type of disease and the
goal of the test.
In general, we will use a sensitive test when:
\begin{itemize}
\item The disease is serious and it is important to detect it. 
\item The disease is curable. 
\item The false positives do not provoke serious traumas.
\end{itemize}

And we will use a specific test when:
\begin{itemize}
\item The disease is important but difficult or impossible to cure.
\item The false positives provoke serious traumas. 
\item The treatment of false positives can have dangerous consequences.  
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Predictive values of a diagnostic test}
But the most important aspect of a diagnostic test is its predictive power, that is measured with the following two posterior probabilities.
\begin{definition}[Positive predictive value $PPV$]
The \emph{positive predictive value} of a diagnostic test is the proportion of persons with the disease to persons with a positive outcome
\[
P(D|+) = \frac{TP}{TP+FP}
\]
\end{definition}

\begin{definition}[Negative predictive value $NPV$]
The \emph{negative predictive value} of a diagnostic test is the proportion of persons without the disease to persons with a negative outcome 
\[
P(\overline{D}|-) = \frac{TN}{TN+FN}
\]
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Predictive values interpretation}
Positive and negative predictive values allow to confirm or to rule out the disease, respectively, if they reach at least a threshold of $0.5$.
\[
\begin{array}{rcl}
PPV>0.5 & \Rightarrow & \mbox{Disease diagnostic}\\
NPV>0.5 & \Rightarrow & \mbox{Not disease diagnostic} 
\end{array}
\]

However, these probabilities depends on prevalence of the disease $P(D)$.
They can be calculated from the sensitivity and the specificity of the diagnostic test using the Bayes theorem.

\begin{align*}
PPV=P(D|+) &= \frac{P(D)P(+|D)}{P(D)P(+|D)+P(\overline{D})P(+|\overline{D})}\\
NPV=P(\overline{D}|-) &= \frac{P(\overline{D})P(-|\overline{D})}{P(D)P(-|D)+P(\overline{D})P(-|\overline{D})}
%  = \frac{\mbox{Prevalence}\cdot
% \mbox{Sensitivity}}{\mbox{Prevalence}\cdot \mbox{Sensitivity}+(1-\mbox{Prevalence})\cdot (1-\mbox{Specificity})}
\end{align*}

Thus, with frequent diseases, the positive predictive value increases, and with rare diseases, the negative predictive value increases. 
\end{frame}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Diagnostic tests}
\framesubtitle{Example}
A diagnostic test for the flu has been tried in a random sample of 1000 persons.
The results are summarized in the table below.
\begin{center}
\begin{tabular}{|m{2.5cm}|m{3cm}<{\centering}|m{3cm}<{\centering}|}
\cline{2-3}
\multicolumn{1}{c|}{} & Presence of flu $D$ & Absence of flu $\overline D$\\ \hline
Test outcome $+$ & 95 & 90 \\
\hline
Test outcome $-$ & 5 & 810 \\
\hline
\end{tabular}
\end{center}

According to this sample, the prevalence of the flu can be estimated as
\[
P(D) = \frac{95+5}{1000} = 0.1.
\] 

The sensitivity of this diagnostic test is
\[
P(+|D) = \frac{95}{95+5}= 0.95. 
\] 

And the specificity is 
\[
P(-|\overline{D}) = \frac{810}{90+810}=0.9.
\]
\end{frame}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Diagnostic tests}
\framesubtitle{Example cont.}
The predictive positive value of the diagnostic test is
\[
PPV = P(D|+) = \frac{95}{95+90} = 0.5135.
\]

As this value is over $0.5$, this means that we will diagnose the flu if the outcome of the test is positive. 
However, the confidence in the diagnostic will be low, as this value is pretty close to $0.5$.

On the other hand, the predictive negative value is 
\[
NPV = P(\overline{D}|-) = \frac{810}{5+810} = 0.9939. 
\]

As this value is almost 1, that means that is almost sure that a person does not have the flu if he or she gets a
negative outcome in the test. 

Thus, this test is a powerful test to rule out the flu, but not so powerful to confirm it.    
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Likelihood ratios of a diagnostic test}
The following measures are usually derived from sensitivity and specificity.
\begin{definition}[Positive likelihood ratio $LR+$]
The \emph{positive likelihood ratio} of a diagnostic test is the ratio between the probability of positive outcomes in
persons with the disease and healthy persons respectively,
\[
LR+=\frac{P(+|D)}{P(+|\overline{D})} = \frac{\mbox{Sensitivity}}{1-\mbox{Specificity}}
\]
\end{definition}

\begin{definition}[Negative likelihood ratio $LR-$]
The \emph{negative likelihood ratio} of a diagnostic test is the ratio between the probability of negative outcomes in
persons with the disease and healthy persons respectively,
\[
LR-=\frac{P(-|D)}{P(-|\overline{D})} = \frac{1-\mbox{Sensitivity}}{\mbox{Specificity}}
\]
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Likelihood ratios interpretation}
Positive likelihood ratio can be interpreted as the number of times that a positive outcome is more probable in people
with the disease than in people without it. 

On the other hand, negative likelihood ratio can be interpreted as the number of times that a negative outcome is more
probable in people with the disease than in people without it. 

Post-test probabilities can be calculated from pre-test probabilities through likelihood ratios.

\[
P(D|+) = \frac{P(D)P(+|D)}{P(D)P(+|D)+P(\overline{D})P(+|\overline{D})} = \frac{P(D)LR+}{1-P(D)+P(D)LR+}
\]

Thus, 
\begin{itemize}
\item A likelihood ratio greater than 1 increases the probability of disease.
\item A likelihood ratio less than 1 decreases the probability of disease.
\item A likelihood ratio 1 does not change the pre-test probability. 
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Likelihood ratios interpretation}
\begin{center}
\tikzsetnextfilename{probability/likelihood_ratios}
\mode<article>{\resizebox{0.6\textwidth}{!}{\input{img/probability/likelihood_ratios}}}
\mode<presentation>{\resizebox{0.95\textwidth}{!}{\input{img/probability/likelihood_ratios}}}
\end{center}
\end{frame}
