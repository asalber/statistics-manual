% Author: Alfredo Sánchez Alberca (asalber@gmail.com)
\section{Frequency distributions: Tabulation and charts}

\mode<presentation>{
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency distribution: Tabulation and charts}
\tableofcontents[sectionstyle=show/hide,hideothersubsections]
\end{frame}
}


%---------------------------------------------------------------------slide----
\begin{frame} 
\frametitle{Descriptive Statistics}
Descriptive Statistics is the part of Statistics in charge of representing, analysing and summarizing the information
contained in the sample.

After the sampling process, is the next step in every statistical study and usually consists of:
\begin{enumerate}
\item Classify, group and sort the data of the sample.
\item Tabulate and plot data according to their frequencies.
\item Calculate numerical measures that summarize the information contained in the sample (\emph{sample statistics}).
\end{enumerate} 

It has no inferential power $\Rightarrow$ \alert{\emph{Do not generalize to the population!}} 
\end{frame}


\subsection{Frequency distribution}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sample classification}
The study of a statistical variable starts measuring the variable in the individuals of the sample and classifying the
values.

There are two ways of classifying data:
\begin{description}
\item[Non-grouping] Sort values from lowest to highest value (if there is an order).
Used with qualitative variables and discrete variables with few distinct values.
\item[Grouping] Group values in intervals (classes) and sort them from lowest to highest intervals. 
Used with continuous variables and discrete variables with many distinct values. 
\end{description}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sample classification}
$X=$Height
\begin{center}
\scalebox{0.6}{\input{img/descriptive/sample_classification}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency count}
$X=$Height
\begin{center}
\scalebox{0.6}{\input{img/descriptive/frequency_count}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sample frequencies}
\begin{definition}[Sample frequencies]
Given a sample of $n$ values of a variable $X$, for every value $x_i$ of the variable
is defined 
\begin{itemize}
\item \structure{Absolute frequency $n_i$}: Is the number of times that value $x_i$ appears in the sample.
\item \structure{Relative frequency $f_i$}: Is the proportion of times that value $x_i$ appears in the sample.
\[
f_i = \frac{n_i}{n}
\]
\item \structure{Cumulative absolute frequency $N_i$}: Is the number of values in the sample less than or equal to
$x_i$.
\[
N_i = n_1 + \cdots + n_i
\]
\item \structure{Cumulative relative frequency $F_i$}: Is the proportion of values in the sample less than or equal to
$x_i$.
\[
F_i = \frac{N_i}{n}
\]
\end{itemize}
\end{definition}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency table}
The set of values of a variable with their respective frequencies is called \structure{\textbf{frequency distribution}}
of the variable in the sample, and it is usually represented as a \structure{\textbf{frequency table}}.
\begin{center}
\begin{tabular}{|>{\centering}p{1.8cm}|>{\centering}p{1.8cm}|>{\centering}p{1.8cm}|>{\centering}p{1.8cm}|p{1.8cm}<{\centering}|}
\hline
\structure{$X$ values} & \structure{Absolute frequency} & \structure{Relative frequency} & \structure{Cumulative
absolute frequency} & \structure{Cumulative relative frequency} \\
\hline
$x_1$ & $n_1$ & $f_1$ & $N_1$ & $F_1$\\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
$x_i$ & $n_i$ & $f_i$ & $N_i$ & $F_i$\\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
$x_k$ & $n_k$ & $f_k$ & $N_k$ & $F_k$\\
\hline
\end{tabular}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency table}
\framesubtitle{Example of quantitative variable and non-grouped data}
The number of children in 25 families are:
\begin{center}
1, 2, 4, 2, 2, 2, 3, 2, 1, 1, 0, 2, 2, \\
 0, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 2
\end{center}
The frequency table for the number of children in this sample is 
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrrrr}
\hline
x_i & n_i & f_i & N_i & F_i\\
\hline
0 & 2 & 0.08 & 2 & 0.08\\
1 & 6 & 0.24 & 8 & 0.32\\
2 & 14 & 0.56 & 22 & 0.88\\
3 & 2  & 0.08 & 24 & 0.96\\
4 & 1 & 0.04 & 25 & 1 \\
\hline
\sum & 25 & 1 \\
\hline
\end{array}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency table}
\framesubtitle{Example of quantitative variable and grouped data}
The heights (in cm) of 30 students are:
\begin{center}
179, 173, 181, 170, 158, 174, 172, 166, 194, 185,\\
162, 187, 198, 177, 178, 165, 154, 188, 166, 171,\\
175, 182, 167, 169, 172, 186, 172, 176, 168, 187.
\end{center}
The frequency table for the height in this sample is
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrrrr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{f_i} & \multicolumn{1}{c}{N_i} & \multicolumn{1}{c}{F_i}\\
\hline
(150,160] & 2 & 0.07 & 2 & 0.07\\
(160,170] & 8 & 0.27 & 10 & 0.34\\
(170,180] & 11 & 0.36 & 21 & 0.70\\
(180,190] & 7  & 0.23 & 28 & 0.93\\
(190,200] & 2 & 0.07 & 30 & 1 \\
\hline
\sum & 30 & 1 \\
\hline
\end{array}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Classes construction}
Intervals are known as \structure{\textbf{classes}} and the center of intervals as \structure{\textbf{class mark}}.

When grouping data into intervals, the following rules must be taken into account: 
\begin{itemize}
\item The number of intervals should not be too big nor too small. 
A usual rule of thumb is to take a number of intervals approximately $\sqrt{n}$ or $\log_2(n)$.
\item The intervals must not overlap and must cover the entire range of values.
It doesn't matter if intervals are left-open and right-closed or vice versa. 
\item The minimum value must fall in the first interval and the maximum value in the last.
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency table}
\framesubtitle{Example with qualitative variable}
The blood type of 30 people are:
\begin{center}
A, B, B, A, AB, 0, 0, A, B, B, A, A, A, A, AB,\\
A, A, A, B, 0, B, B, B, A, A, A, 0, A, AB, 0.
\end{center}
The frequency table of the blood type is 
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{crr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{f_i} \\
\hline
\mbox{0} & 5 & 0.16 \\
\mbox{A} & 14 & 0.47 \\
\mbox{B} & 8 & 0.27 \\
\mbox{AB} & 3 & 0.10 \\
\hline
\sum & 30 & 1 \\
\hline
\end{array}
\]
\begin{center}
\emph{Why there are not cumulative frequencies?}
\end{center} 
\end{frame}


\subsection{Frequency distribution graphs}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Frequency distribution graphs}
Usually the frequency distribution is also displayed graphically.
 
Depending on the type of variable and if data has been grouped or not, there are different types of charts:
\begin{itemize}
\item Bar chart
\item Histogram
\item Line chart or ogive. 
\item Pie chart
\end{itemize}
\end{frame} 


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Bar chart}
A \structure{bar chart} consists in a set of bars, one for every value or category of the variable, plotted on a
coordinate system.

Usually the values or categories of the variable are represented on the $x$-axis, and the frequencies on the $y$-axis. 
For each value or category of the variable, a bar is draw to the height of its frequency.
The width of the bar is not important but bars should be clearly separated among them. 

Depending on the type of frequency represented in the $y$-axis we get different types of bar charts.
 
Sometimes a polygon, known as \structure{\textbf{frequency polygon}}, is plotted joining the top of every bar.
\end{frame}



%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Absolute frequency bar chart}
\framesubtitle{Non-grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/abs_freq_bar_chart}} 
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Absolute frequency line chart or polygon}
\framesubtitle{Non-grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/abs_freq_bar_chart_polygon}} 
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative absolute frequency bar chart}
\framesubtitle{Non-grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_abs_freq_bar_chart}} 
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative absolute frequency line chart or polygon}
\framesubtitle{Non-grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_abs_freq_bar_chart_polygon}} 
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Histogram}
A \structure{histogram} is similar to a bar chart but for grouped data.  

Usually the classes or grouping intervals are represented on the $x$-axis, and the frequencies on the $y$-axis. 
For each class, a bar is draw to the height of its frequency.
Contrary to bar charts, the width of bars coincides with the width of classes, and there are no space between two
consecutive bars.

Depending on the type of frequency represented in the $y$-axis we get different types of histograms.
 
Sometimes a polygon, known as \structure{\textbf{frequency polygon}}, is plotted joining the top of every bar.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Absolute frequency histogram}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/abs_freq_histogram}}
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Absolute frequency histogram}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/abs_freq_histogram_polygon}} 
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative absolute frequency histogram}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_abs_freq_histogram}}
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative absolute frequency line chart or ogive}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_abs_freq_histogram_polygon}} 
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative relative frequency histogram}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_rel_freq_histogram}}
\end{center} 
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Cumulative relative frequency line chart or ogive}
\framesubtitle{Grouped data}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/cum_rel_freq_histogram_polygon}} 
\end{center} 
\end{frame}
 

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Pie chart}
A \structure{pie chart} consists in a circle divided in slices, one for every value or category of the variable. 
Each slice is called \structure{sector} and its angle or area is proportional to the frequency of the corresponding
value or category. 

Pie charts can represent absolute or relative frequencies, but not cumulative frequencies, and are used with nominal
qualitative variables.
For ordinal qualitative or quantitative variables is better to use bar charts or histograms, cause it's easy to perceive
differences in one dimension (lenght of bars) than in two dimensions (areas of sectors).
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Pie chart}
\framesubtitle{Nominal variables}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/rel_freq_pie_chart}}
\end{center}
\end{frame}


% ---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Outliers}
One of the main problems in samples are \structure{\textbf{outliers}}, that are values very different from the rest of
values of the sample.
\begin{center}
\includegraphics[scale=0.5]{img/descriptive/outlier.png}
\end{center}

It's important to find out outliers before doing any analysis, cause \alert{\emph{outliers usually distort the
results}}.

They always appears in the ends of the distribution, and can be find out easily with a box and whiskers chart (as 
be showed later).
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Outliers management}
With big samples outliers have less importance and can be left in the sample.

With small samples we have several options:

\begin{itemize}
\item Remove the outlier if it's an error. 
\item Replace the outlier by the lower or higher value in the distribution that is not an outlier if it's not an error
and the outlier doesn't fit the theoretical distribution. 
\item Leave the outlier if it's not an error, and change the theoretical model to fit it to outliers.  
\end{itemize}
\end{frame}
 


\subsection{Sample statistics}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Sample statistics}
The frequency table and charts summarize and give an overview of the distribution of values of the studied variable in
the sample, but it's difficult to describe some aspects of the distribution from it, as for example, which are the most
representative values of the distribution, how is the spread of data, which data could be considered outliers, how is
the symmetry of the distribution. 

To describe those aspects of the sample distribution more specific numerical measures, called
\structure{\textbf{sample statistics}}, are used.

According to the aspect of the distribution that they study, there are different types of statistics:
\begin{description}
\item[Measures of locations:] They measure the values where data are concentrated or that divide the distribution into
equal parts. 
\item[Measures of dispersion:] They measure the spread of data.
\item[Measures of shape:] They measure the symmetry and kurtosis of the distribution.  
\end{description}
\end{frame}


\subsection{Location statistics}

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Location statistics}
There are two groups: 

\begin{description}
\item [Central location measures:] They measure the values where data are concentrated, and that usually are in the
centre of the distribution. 
These values are the values that best represents the sample data. 
The most important are:
\begin{itemize}
\item Arithmetic mean
\item Median
\item Mode
\end{itemize}
\item [Non-central location measures:] They divide the sample data into equals parts. 
The most important are:
\begin{itemize}
\item Quartiles.
\item Deciles.
\item Percentiles. 
\end{itemize}
\end{description}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Arithmetic mean}
\begin{definition}[Sample arithmetic mean $\bar{x}$]
The \emph{sample arithmetic mean} of a variable $X$ is the sum of observed values in the sample divided by the sample
size
\[
\bar{x} = \frac{\sum x_i}{n}
\]
\end{definition}
From the frequency table can be calculated with the formula
\[
\bar{x} = \frac{\sum x_in_i}{n} = \sum x_i f_i
\]

In most cases the arithmetic mean is the value that best represent the observed values in the sample. 
\begin{center}
\alert{\emph{Watch out! It can not be calculated with qualitative variables.}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Arithmetic mean calculation}
\framesubtitle{Example with non-grouped data}
Using the data of the sample with the number of children of families, the arithmetic mean is 
\begin{align*}
\bar{x} &= \frac{1+2+4+2+2+2+3+2+1+1+0+2+2}{25}+\\
&+\frac{0+2+2+1+2+2+3+1+2+2+1+2}{25} = \frac{44}{25} = 1.76 \mbox{ hijos}.
\end{align*}
or using the frequency table
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrrrr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{f_i} & \multicolumn{1}{c}{x_in_i} & \multicolumn{1}{c}{x_if_i}\\
\hline
0 & 2 & 0.08 & 0 & 0\\
1 & 6 & 0.24 & 6 & 0.24\\
2 & 14 & 0.56 & 28 & 1.12\\
3 & 2  & 0.08 & 6 & 0.24\\
4 & 1 & 0.04 & 4 & 0.16 \\
\hline
\sum & 25 & 1 & 44 & 1.76 \\
\hline
\end{array}
\]
\[
\bar{x} = \frac{\sum x_in_i}{n} = \frac{44}{25}= 1.76 \qquad \bar{x}=\sum{x_if_i} = 1.76.
\]
That means that the value that best represent the number of children in the families of the sample is $1.76$ children.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Arithmetic mean calculation}
\framesubtitle{Example with grouped data}
Using the data of the sample of student heights, the arithmetic mean is 
\[
\bar{x} = \frac{179+173+\cdots+187}{30} = 175.07 \mbox{ cm}.
\]
or using the frequency table with the class marks 
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrrrrr}
\hline
\multicolumn{1}{c}{X} & \multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{f_i} & \multicolumn{1}{c}{x_in_i} & \multicolumn{1}{c}{x_if_i}\\
\hline
(150,160] & 155 & 2 & 0.07 & 310 & 10.33\\
(160,170] & 165 & 8 & 0.27 & 1320 & 44.00\\
(170,180] & 175 & 11 & 0.36 & 1925 & 64.17\\
(180,190] & 185 & 7 & 0.23 & 1295 & 43.17\\
(190,200] & 195 & 2 & 0.07 & 390 & 13 \\
\hline
\sum &  & 30 & 1 & 5240 & 174.67 \\
\hline
\end{array}
\]
\[
\bar{x} = \frac{\sum x_in_i}{n} = \frac{5240}{30}= 174.67 \qquad \bar{x}=\sum{x_if_i} = 174.67.
\]

Observe that when the mean is calculated from the table the result differs a little from the real value, cause the
values used in the calculations are the class marks instead of the actual values.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Weighted mean}
In some cases the values of the sample have different importance. 
In that case the importance or \emph{weight} of each value of the sample must be taken into account when calculating
the mean. 

\begin{definition}[Sample weighted mean $\bar{x}_p$]
Given a sample of values $x_1,\ldots,x_n$ where every value $x_i$ has a weight $p_i$, the \emph{weighted
mean} of variable $X$ is the sum of the product of each value by its weight, divided by sum of weights
\[
\bar{x}_p = \frac{\sum x_ip_i}{\sum p_i}
\]
\end{definition}

From the frequency table can be calculated with the formula
\[
\bar{x}_p = \frac{\sum x_ip_in_i}{\sum p_i}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Weighted mean calculation}
Assume that a student wants to calculate a representative measure o its performance in a course. 
The grade and the credits of every subjects are 
\begin{center}
\begin{tabular}{lcc}
\hline
Subject & Credits & Grade\\
\hline
Maths & 6 & 5 \\
Economics & 4 & 3 \\
Chemistry & 8 & 6 \\
\hline
\end{tabular}
\end{center}
The arithmetic mean is 
\[
\bar{x} = \frac{\sum x_i}{n} = \frac{5+3+6}{3}= 4.67 \text{ points},
\]
However, this measure does not represent well the performance of the student, as not all the subjects have the same
importance and require the same effort to pass. 
Subjects with more credits require more work and must have more weight in the calculation of the mean. 

In this case is better to use the weighted mean, using the credits as the
weights of grades, as a representative measure of the student effort
\[
\bar{x}_p = \frac{\sum x_ip_i}{\sum p_i} = \frac{5\cdot 6+3\cdot 4+6\cdot 8}{6+4+8}= \frac{90}{18} = 5 \text{ points}.
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Median}
\begin{definition}[Sample median $Me$]
The \emph{sample median} of a variable $X$ is the value that is in the middle of the ordered sample. 
\end{definition}

The median divides the sample distribution in into two equal parts, that is, there are the same number of values above
and below the median.
It has cumulative frequencies $N_{Me}= n/2$ y  $F_{Me}= 0.5$.

\begin{center}
\alert{\emph{Watch out! It can not be calculated for nominal variables.}}
\end{center}

\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Median calculation}
\framesubtitle{Non-grouped data}
With non-grouped data, there are two possibilities:
\begin{itemize}
\item Odd sample size: The median is the value in the position $\frac{n+1}{2}$.
\item Even sample size: The median is the average of values in positions $\frac{n}{2}$ and $\frac{n}{2}+1$.
\end{itemize}
\begin{center}
\scalebox{0.4}{\input{img/descriptive/median}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Median calculation}
\framesubtitle{Example with non-grouped data}
Using the data of the sample with the number of children of families, the sample size is 25, that is odd, and the median
is the value in the position $\frac{25+1}{2} = 13$ of the sorted sample. 
\[
0,0,1,1,1,1,1,1,2,2,2,2,\fbox{2},2,2,2,2,2,2,2,2,2,3,3,4
\]
and the median is 2 children.

With the frequency table, the median is the lowest value with a cumulative absolute frequency greater than or equal to
$13$, or with a cumulative relative frequency greater than or equal to $0.5$.
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrrrr}
\hline
x_i & n_i & f_i & N_i & F_i\\
\hline
0 & 2 & 0.08 & 2 & 0.08\\
1 & 6 & 0.24 & 8 & 0.32\\
\rowcolor{coral} \color{color1}2 & 14 & 0.56 & 22 & 0.88\\
3 & 2  & 0.08 & 24 & 0.96\\
4 & 1 & 0.04 & 25 & 1 \\
\hline
\sum & 25 & 1 \\
\hline
\end{array}
\]
\end{frame}

% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo de la mediana con datos agrupados}
% Con datos agrupados la mediana se calcula interpolando en el polígono de frecuencias absolutas acumuladas para el valor $n/2$.
% \begin{center}
% \scalebox{0.7}{\input{img/descriptive/calculo_mediana_datos_no_agrupados}}
% \end{center}
% \note{Cuando se trabaja con datos agrupados en clases, la mediana se calcula de forma aproximada, interpolando en el
% polígono de frecuencias acumuladas para el valor $n/2$. 
% 
% La interpolación consiste en proyectar sobre el polígono la frecuencia $n/2$ y ver a qué altura del eje de abscisas
% corta al polígono de frecuencias. Dicho valor es la mediana.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpolación en el polígono de frecuencias absolutas acumuladas}
% \begin{center}
% \scalebox{1}{\input{img/descriptive/interpolacion}}
% \end{center}
% 
% \uncover<3->{
% \[
% Me = l_{i-1}+\frac{n/2 - N_{i-1}}{N_i-N_{i-1}}(l_i-l_{i-1}) = l_{i-1}+\frac{n/2-N_{i-1}}{n_i}a_i
% \]
% }
% 
% \note{La interpolación en realidad consiste en una razón de semejanza de triángulos. 
% 
% En primer lugar se identifica el intervalo en el que cae la mediana, mirando en la columna de frecuencias acumuladas de
% la tabla de frecuencias, de igual modo a como se hace para datos no agrupados. Una vez identificado el intervalo, se
% toma el segmento del polígono de frecuencias acumuladas que corresponde a dicho intervalo. Supongamos que dicho
% intervalo tiene límite inferior $l_{i-1}$ y límite superior $l_i$, y que parte de una frecuencia absoluta acumulada
% $N_{i-1}$ y llega a una frecuencia absoluta acumulada $N_{i}$. Este segmento define un triángulo rectángulo de ángulo
% $\alpha$ cuya tangente es el cateto opuesto, que vale $N_i-N_{i-1}$ entre el cateto contiguo, que es precisamente la
% amplitud del intervalo $l_i-l_{i-1}$.
% 
% Por otro lado, si proyectamos la frecuencia corresondiente a la media $n/2$ sobre el polígono, en el punto de corte
% aparecería la mediana, de manera se se tiene otro triángulo rectángulo más pequeño que es semejante al anterior al
% compartir el mismo ángulo $\alpha$. Al igual que antes, la tangente de este ángulo será el cateto opuesto, que ahora
% vale $n/2-N_i$ entre el cateto contiguo que ahora vale $Me-l_{i-1}$. 
% 
% Puesto que se trata del mismo ángulo, su tangente es la misma y se pueden igual ambas expresiones, dando lugar a una
% ecuación conde la única incógnita es la mediana. Despejandola, se obtiene la fórmula para calcular la mediana.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo de la mediana}
% \framesubtitle{Ejemplo con datos agrupados}
% En el ejemplo de las estaturas $n/2 =
% 30/2 = 15$. Si miramos en el polígono de frecuencias acumuladas comprobamos que
% la mediana caerá en el intervalo $(170,180]$.
% \begin{center}
% \scalebox{0.7}{\input{img/descriptive/interpolacion_ejemplo_1}}
% \end{center}
% \note{Veamos un ejemplo de interpolación para calcular la mediana de la muestra de estaturas. Mirando en la tabla de
% frecuencias se observa que el primer intervalo con una frecuencia igual o mayor que $n/2=30/2=15$ es el que va de 170
% cm a 180 cm, así que se toma el trozo del polígono de frecuencias absolutas acumuladas corresondiente a este intervalo.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpolación en el polígono de frecuencias absolutas acumuladas}
% \begin{center}
% \scalebox{1}{\input{img/descriptive/interpolacion_ejemplo_2}}
% \end{center}
% 
% \uncover<3->{
% \[
% Med = 170+\frac{15 - 10}{21-10}(180-170) = 170+\frac{5}{11}10 = 174.54
% \]
% }
% 
% \note{Si nos fijamos en el triángulo grande, la tangente de $\alpha$ vale $21-10$ que es el cateto opuesto, entre
% $180-170$ que es el cateto contiguo, mientras que si nos fijamos en el triángulo pequeño que aparece al proyectar $15$
% sobre el polígono, se tiene que la tangente de $\alpha$ vale $15-10$ que es cateto opuesto entre $Me-170$ que es el
% cateto contiguo. Igualando ambas expresiones y despejando la mediana se obtiene $174.54$ cm que es la estatura mediana.
% 
% Una comprobación que conviene hacer siempre es ver que el valor obtenido cae efectivamente dentro del intervalo de
% interpolación.}
% \end{frame}
% 

%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Mode}
\begin{definition}[Sample Mode $Mo$]
The \emph{sample mode} of a variable $X$ is the most frequent value in the sample.
\end{definition}

With grouped data the \emph{modal class} is the class with the highest frequency. 

It can be calculated for all types of variables (qualitative and quantitative). 

Some distributions can have more than one mode
\begin{center}
\scalebox{0.4}{\input{img/descriptive/mode}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Mode calculation}
Using the data of the sample with the number of children of families, the value with the highest frequency is $2$, that
is the mode $Mo = 2$ children.
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} \\
\hline
0 & 2 \\
1 & 6 \\
\rowcolor{coral}\color{color1} 2 & 14 \\
3 & 2  \\
4 & 1 \\
\hline
\end{array}
\]

Using the data of the sample of student heights, the class with the highest frequency is $(170,180]$ that is the modal
class $Mo=(170,180]$.
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} \\
\hline
(150,160] & 2 \\
(160,170] & 8 \\
\rowcolor{coral} \color{color1}(170,180] & 11 \\
(180,190] & 7 \\
(190,200] & 2 \\
\hline
\end{array}
\]
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Which central tendency statistic should I use?}
In general, when all the central tendency statistics can be calculated, is advisable to use them as representative
values in the following order:
\begin{enumerate}
\item Mean. Mean takes more information from the sample than the others, as it takes into account the magnitude
of data.
\item Median. Median takes less information than mean but more than mode, as it takes into account the order
of data.
\item Mode. Mode is the measure that fewer information takes from the sample, as it only takes into account the
absolute frequency of values.
\end{enumerate}

But, \emph{be careful with outliers}, as the mean can be distorted by them.
In that case is better to use the median as the value most representative.

For example, if a sample of number of children of 7 families is
\begin{center}
0, 0, 1, 1, 2, 2, 15

$\bar{x}=3$ children \quad and \quad $Me=1$ children

\emph{Which measure represent better the number of children in the sample?}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Non-central location measures}
The non-central location measures or \emph{quantiles} divide the sample distribution in equal parts.

The most used are:
\begin{description}
\item[Quartiles:] Divide the distribution into 4 equal parts. 
There are 3 quartiles: $C_1$ (25\% acumulated) , $C_2$ (50\% acumulated), $C_3$ (75\% acumulated).
\item[Deciles:] Divide the distribution into 10 equal parts.\\
There are 9 deciles: $D_1$ (10\% acumulated) ,\ldots, $D_9$ (90\% acumulated).
\item[Percentiles:] Divide the distribution into en 100 equal parts.\\
There are 99 percentiles: $P_1$ (1\% acumulated),\ldots, $P_{99}$ (99\% acumulated).
\end{description}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Quantiles}
\begin{center}
\scalebox{0.8}{\input{img/descriptive/quantiles}}
\end{center}
\onslide<4->{
Observe that there is a correspondence between quartiles, deciles and percentiles. 
For example, first quartile coincide with percentile 25, and fourth decile coincides with the percentile 40.}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Quantiles calculation}
Quantiles are calculated in a similar way to the median. 
The only difference lies in the cumulative relative frequency that correspond to every quantile.  
\begin{center}
\scalebox{0.6}{\input{img/descriptive/quantiles_calculation}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Quantile calculation}
\framesubtitle{Example with non-grouped data}
Using the data of the sample with the number of children of families, the cumulative relative frequencies were
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{F_i} \\
\hline
0 & 0.08\\
1 & 0.32\\
2 & 0.88\\
3 & 0.96\\
4 & 1\\
\hline
\end{array}
\]

\begin{align*}
F_{C_1}=0.25 &\Rightarrow C_1 = 1 \text{ children},\\
F_{C_2}=0.5 &\Rightarrow C_2 = 2 \text{ children},\\
F_{C_3}=0.75 &\Rightarrow C_3 = 2 \text{ children},\\
F_{D_4}=0.4 &\Rightarrow D_3 = 2 \text{ children},\\
F_{P_{92}}=0.92 &\Rightarrow P_{92} = 3 \text{ children}.\\
\end{align*}
\end{frame}


\subsection{Dispersion statistics}
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Dispersion statistics}
\emph{Dispersion} or \emph{spread} refers to the variability of data. 
So, dispersion statistics measure how the data values are scattered in general, or with respect to a central location
measure. 

For quantitative variables, the most important are:
\begin{itemize}
\item Range
\item Interquartile range
\item Variance
\item Standard deviation
\item Coefficient of variation
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Range and interquartile range}
\begin{definition}[Sample range]
The \emph{sample range} of a variable $X$ is the difference between the the maximum and the minimum value in the sample.
\[\text{Range} = \max_{x_i} -\min_{x_i}\]
\end{definition}

The range measure the largest variation among the sample data. 
However, it's very sensitive to outliers, as they appear at the ends of the distribution, and for that reason is
rarely used. 
\begin{center}
\scalebox{0.8}{\input{img/descriptive/range}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Range and interquartile range}
The following measure avoid the problem of outliers and is much more used.

\begin{definition}[Sample interquartile range]
The \emph{sample interquartile range} of a variable $X$ is the difference between the third and the first
sample quartiles.
\[\text{IQR} = Q_3 -Q_1\]
\end{definition}
\begin{center}
\scalebox{0.8}{\input{img/descriptive/interquartile_range}}
\end{center}

The interquartile range measures the spread of the 50\% central data.
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Box plot}
The dispersion of a variable in a sample can be graphically represented with a \structure{\textbf{box plot}}, that
represent five descriptive statistics (minimum, quartiles and maximum) known as the \emph{five-numbers}.
It consist in a box, drawn from the lower to the upper quartile, that represent the interquartile range, and two
segments, known as the lower and the upper \emph{whiskers}.  
Usually the box is split in two with the median. 

This chart is very helpful as it serves to many purposes:  
\begin{itemize}
\item It serves to measure the spread of data as it represent the range and the interquartile range. 
\item It serves to detect outliers, that are the values outside the interval defined by the whiskers.
\item It serves to measure the symmetry of distribution, comparing the length of the boxes and whiskers above and below
the median. 
\end{itemize}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Box plot}
\framesubtitle{Example with newborn weights}
\begin{center}
\scalebox{0.6}{\input{img/descriptive/boxplot}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Box plot construction}
To create a box plot follow the steps below
\begin{enumerate}
\item Calculate the quartiles. 
\item Draw a box from the lower to the upper quartile. 
\item Split the box with the median or second quartile. 
\item For the whiskers calculate first two values called \emph{fences} $f_1$ y $f_2$. The lower fence is the
lower quartile minus one and a half the interquartile range, and the upper fence is the upper quartile plus one and a
half the interquartile range:
\begin{align*}
f_1&=Q_1-1.5\,\text{IQR}\\
f_2&=Q_3+1.5\,\text{IQR}
\end{align*}
The fences define the interval where data are considered normal.
Any value outside that interval is considered an outlier. \\ 
For the lower whisker draw a segment from the lower quartile to the lower value in the sample grater than or
equal to $f_1$, and for the upper whisker draw a segment from the upper quartile to the highest value in the sample lower than
or equal to $f_2$.
\item Finally, if there are some outlier, draw a dot in every outlier. 
\end{enumerate}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Box plot construction}
\framesubtitle{Example of number of children}
\begin{enumerate}
\uncover<2->{\item Calculate the quartiles: $Q_1=1$ children} \uncover<3->{and $Q_2=Q_3=2$ children}
\uncover<4->{\item Draw the box.}
\uncover<5->{\item Calculate the fences $f_1=1-1.5*1=-0.5$ and $f_2=2+1.5*1=3.5$.}
\uncover<6->{\item Draw the whiskers: $w_1=0$ children} \uncover<7->{and $w_2=3$ children.}
\uncover<8->{\item Draw the outliers: 4 children.}
\end{enumerate}
\begin{center}
\scalebox{0.45}{\input{img/descriptive/boxplot_children}}
\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Deviations from the mean}
Another way of measuring spread of data is with respect to a central tendency measure, as for example the mean. 

In that case, it's measured the distance from every value in the sample to the mean, that is called
\structure{\textbf{deviation from the mean.}}

\begin{center}
\scalebox{1}{\input{img/descriptive/deviations}}
\end{center}

If deviations are big, the mean would be less representative as when they are small.
%\begin{center} 
%\emph{Which mean is more representative?}
%\end{center}
\end{frame}


%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Variance and standard deviation}
\begin{definition}[Sample variance $s^2$]
The \emph{sample variance} of a variable $X$ is the average of squared deviations from the mean. 
\[
s^2 = \frac{\sum (x_i-\bar x)^2n_i}{n} = \sum (x_i-\bar x)^2f_i
\]
\end{definition}
It can also be calculated with the formula
\[
s^2 = \frac{\sum x_i^2n_i}{n} -\bar x^2= \sum x_i^2f_i-\bar x^2
\]
The variance has the units of the variable squared, and to ease their interpretation it's common to calculate its square
root.

\begin{definition}[Sample standard deviation $s$]
The \emph{sample standard deviation} of a variable $X$ is the square root of the variance.
\[
s = +\sqrt{s^2}
\]
\end{definition}
\end{frame}


% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpretación de la varianza y la desviación típica}
% Tanto la varianza como la desviación típica sirven para cuantificar la dispersión de los datos en torno a la media. 
% %Si la dispersión es grande la media será menos representativa de la muestra que si la dispersión es pequeña.
% \begin{center}
% \includegraphics[scale=0.4]{img/descriptive/interpretacion_varianza}
% 
% %\emph{¿En qué caso es más representativa la media?}
% \end{center}
% 
% \note{Tanto la varianza como la desviación típica sirven para cuantificar la dispersión de los datos en torno a la media. Si la dispersion
% con respecto a la media es pequeña, los individuos se parecerán bastante a la media y esta será más representativa que cuando los
% individuos no se parezcana ella y la dispersión con respecto a la media sea mayor.}
% \end{frame}
% 
% 
%---------------------------------------------------------------------slide----
\begin{frame}
\frametitle{Variance and standard deviation calculation}
\framesubtitle{Example with non-grouped data}
Using the data of the sample with the number of children of families, and adding a new column to the frequency table
with the squared values, 
\[
\setlength\arraycolsep{3mm}
\setlength\arrayrulewidth{0.5pt}
\begin{array}{rrr}
\hline
\multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{x_i^2n_i} \\
\hline
0 & 2 & 0 \\
1 & 6 & 6 \\
2 & 14 & 56\\
3 & 2  & 18\\
4 & 1 & 16 \\
\hline
\sum & 25 & 96 \\
\hline
\end{array}
\]
\[
s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{96}{25}-1.76^2= 0.7424 \mbox{ children}^2.
\]
and the standard deviation is $s=\sqrt{0.7424} = 0.8616$ children.

Compared to the range, that is 4 children, the standard deviation is not very large, so we can conclude
that the dispersion of the distribution is small and consequently the mean, $\bar x=1.76$ children, represents quite
well the number of children of families of the sample. 
\end{frame}


% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo de la varianza y la desviación típica}
% \framesubtitle{Ejemplo con datos agrupados}
% En el ejemplo de las estaturas, al ser datos agrupados, el cálculo se realiza igual que antes pero tomando como valores de la variable las
% marcas de clase. 
% \begin{center}
% \setlength\arraycolsep{3mm}
% \setlength\arrayrulewidth{0.5pt}
% \begin{array}{rrrr}
% \hline
% \multicolumn{1}{c}{X} & \multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{x_i^2n_i} \\
% \hline
% (150,160] & 155 & 2 & 48050\\
% (160,170] & 165 & 8 & 217800\\
% (170,180] & 175 & 11 & 336875\\
% (180,190] & 185 & 7 & 239575\\
% (190,200] & 195 & 2 & 76050\\ 
% \hline 
% \sum &  & 30 & 918350 \\
% \hline
% \end{array}
% \end{center}
% \[
% s^2 = \frac{\sum x_i^2n_i}{n}-\bar x^2 = \frac{918350}{30}-174.67^2= 102.06 \mbox{ cm}^2.
% \]
% Y la desviación típica es $s=\sqrt{102.06} = 10.1$ cm.
% 
% Este valor es bastante pequeño, comparado con el recorrido de la variable, que va de 150 a 200 cm, por lo que la variable tiene poca
% dispersión y en consecuencia su media es muy representativa.
% 
% \note{En el ejemplo de las estaturas, como se habían agrupado los datos, como valores se tomarán las
% marcas de clases, es decir, 155 elevado al cuadrado y por su frecuencia absoluta que es 2, lo que nos da 48050, y así sucesivamente. Después
% hay que sumar los valores de esta columna y dividirlos por el tamaño de la muestra que era 30. Finalmente al cociente se le resta el valor
% de la media que era $174.67$ elevada al cuadrado, lo que nos da 102.06 cm al cuadrado.
% 
% Si sacamos la raíz cuadrada se obtiene una desviación típica de 10.1 cm, que es un valor pequeño comparado con el recorrido de la variable
% que va de 150 a 200 cm, por lo que se puede concluir que la muestra tiene poca dispersión y por tanto la media representa muy bien al resto
% de individuos de la muestra.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de variación}
% Tanto la varianza como la desviación típica tienen unidades y eso dificulta a veces su interpretación y su comparación.
% 
% Afortunadamente es fácil definir a partir de ellas una medida de dispersión adimensional que es más fácil de interpretar.
% \begin{definition}[Coeficiente de variación muestral $cv$]
% El \emph{coeficiente de variación muestral} de una variable $X$ se define como el cociente entre su desviación típica muestral y el valor absoluto de su media muestral.
% \[
% cv = \frac{s}{|\bar x|}
% \]
% \end{definition}
% El coeficiente de variación muestral mide la dispersión relativa de los valores de la muestra en torno a la media muestral.
%  
% Como no tiene unidades, es muy sencillo de interpretar: Cuanto mayor sea, mayor será la dispersión y menos
% representativa será la media.
% 
% También se utiliza para comparar la dispersión entre muestras distintas incluso si las variables tienen unidades diferentes.
% \begin{center}
% \alert{\emph{¡Ojo! No tiene sentido cuando la media muestral vale 0 o valores próximos.}}
% \end{center}
% 
% \note{Tanto la varianza como la desviación típica tienen unidades lo que dificulta a veces su interpretación.
% 
% Afortunadamente es fácil definir a partir de ellas una medida de dispersión adimensional que es más fácil de interpretar.
% 
% El coeficiente de variación muestral, que se representa mediante cv, se defiene como el cociente entre la desviación típica y el valor
% absoluto de la media.
% 
% Como tanto la desviación típica como la media tienen las unidades de la variable, al hacer su cociente las unidades desaparecen y se obtiene
% una medida adimensional que resulta más sencilla de interpretar. 
% 
% Al estar dividido por el valor absoluto de la media, el coeficiente de variación mide la dispersión relativa de los valores de la muestra
% en torno a la media muestral, y como en el numerador está la desviación típica, cuanto mayor sea esta, mayor será el coeficiente de
% variación y por tanto mayor será la dispersión relativa de la variable en torno a la media.
% 
% Una de las principales utilidades del coeficiente de variación es que, precisamente por no tener unidades, permite la comparación de la
% dispersión de muestras distintas, incluso si son de variables con distintas unidades. 
% 
% El único problema de este estadístico es que no vale cuando la media muestral vale 0 o próxima a 0, ya que al estar en el denominador,
% obtendríamos valores muy grandes.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de variación}
% \framesubtitle{Ejemplo}
% En el caso del número de hijos, como $\bar x=1.76$ hijos y $s=0.8616$ hijos, se tiene que el coefiente de variación vale
% \[
% cv = \frac{s}{|\bar x|} = \frac{0.8616}{|1.76|} = 0.49.
% \]
% En el caso de las estaturas, como $\bar x=174.67$ cm y $s=10.1$ cm, se tiene que el coeficiente de variación vale 
% \[
% cv = \frac{s}{|\bar x|} = \frac{10.1}{|174.67|} = 0.06.
% \]
% Como se puede observar la dispersión relativa en la muestra de estaturas es mucho menor que en la del número de hijos, por lo que la media
% de las estaturas será más representativa que la media del número de hijos. 
% \end{frame}
% 
% 
% \subsection{Estadísticos de forma}
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Estadísticos de forma}
% Son medidas que tratan de caracterizar aspectos de la forma de la distribución de una muestra.
% 
% Los aspectos más relevantes son:
% \begin{description}
% \item[Simetría:] Miden la simetría de la distribución de frecuencias en torno a la media.\\
% El estadístico más utilizado es el \emph{Coeficiente de Asimetría de Fisher}.
% \item[Apuntamiento:] Miden el apuntamiento de la distribución de frecuencias.\\
% El estadístico más utilizado es el \emph{Coeficiente de Apuntamiento o Curtosis}.
% \end{description}
% \note{Los estadísticos de forma se encargan de describir, como su propio nombre indica, la forma que tiene la distribución de valores en la
% muestra, en particular se estudian dos aspectos que son la asímetría y el apuntamiento.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de asimetría}
% \begin{definition}[Coeficiente de asimetría muestral $g_1$]
% El \emph{coeficiente de asimetría muestral} de una variable $X$ se define como el promedio de las desviaciones de los
% valores de la muestra respecto de la media muestral, elevadas al cubo, dividido por la desviación típica al cubo. \[g_1
% = \frac{\sum (x_i-\bar x)^3 n_i/n}{s^3} = \frac{\sum (x_i-\bar x)^3 f_i}{s^3}\]
% \end{definition}
% El coeficiente de asimetría muestral mide el grado de simetría de los valores de la muestra con respecto a la media muestral, de manera que:
% \begin{itemize}
% \item $g_1=0$ indica que hay el mismo número de valores a la derecha y a la izquierda de la media (simétrica).
% \item $g_1<0$ indica que la mayoría de los valores son mayores que la media (asimétrica a la izquierda).
% \item $g_1>0$ indica que la mayoría de los valores son menores que la media (asimétrica a la derecha).
% \end{itemize}
% 
% \note{La simetría con respecto a la media tiene que ver con la ubicación de los valores a un lado y otro de la media, cuántos valores hay
% por encima y cuántos por debajo, y cómo están de alejados.
% 
% El coeficiente de asimetría muestral, que se representa $g_1$ se define, como la suma del producto de las desviaciones de los valores de la
% muestra a la media muestral elevadas al cubo por su frecuencia absoluta, dividida por el tamaño de la muestra, y a su vez todo dividido
% por la desviación típica al cubo.
% 
% Como las desviaciones elevadas al cubo tienen las unidades de la variable al cubo y la desviación típica elevada al cubo también tiene las
% unidades de la variable al cubo, al realizar el cociente las unidades se cancelan y por tanto el coeficiente de asimetría es una medida
% adimensional que mide el grado de asimetría de los valores de la muestra con respecto a la media, de manera que:
% \begin{itemize}
% \item $g_1=0$ indica que hay el mismo número de valores a la derecha y a la izquierda de la media y por tanto la distribución es simétrica.
% \item $g_1<0$ indica que la mayoría de los valores son mayores que la media y entonces se dice que la distribución es asimétrica hacia la
% izquierda.
% \item $g_1>0$ indica que la mayoría de los valores son menores que la media y entonces se dice que la distribución es asimétrica hacia la
% derecha.
% \end{itemize}
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de asimetría}
% \framesubtitle{Ejemplo de distribución simétrica}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_simetrica}}
% \end{center}
% \note{Aquí tenemos el histograma de una distribución simétrica. Como puede observarse, la media queda justo en el centro de la
% distribución, coincidiendo con la mediana y existe el mismo número de barras y con la misma frecuencia a un lado y a otro de la media.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de asimetría}
% \framesubtitle{Ejemplo de distribución asimétrica hacia la izquierda}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_asimetrica_izquierda}}
% \end{center}
% \note{En este otro caso tenemos una distribución asimétrica hacia la izquierda, donde la media queda por debajo de la mediana y las barras
% son más altas a la derecha de la media, lo que indica que hay más valores por encima de la media. Por debajo de la media habría menos
% valores, barras más bajas, pero más alejados.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de asimetría}
% \framesubtitle{Ejemplo de distribución asimétrica hacia la derecha}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_asimetrica_derecha}}
% \end{center}
% \note{Y en este otro caso tenemos una distribución asimétrica hacia la derecha, donde la media queda por encima de la mediana y las barras
% son más altas a la izquierda de la media, lo que indica que hay más valores por debajo de la media. Por encima de la media habría menos
% valores, barras más bajas, pero más alejados.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo del coeficiente de asimetría}
% \framesubtitle{Ejemplo con datos agrupados}
% Siguiendo con el ejemplo de las estaturas, podemos calcular el coeficiente de asimetría a partir de la tabla de frecuencias añadiendo una
% nueva columna con los cubos de las desviaciones a la media $\bar x = 174.67$ cm:
% \begin{center}
% \setlength\arraycolsep{3mm}
% \setlength\arrayrulewidth{0.5pt}
% \begin{array}{rrrrr}
% \hline
% \multicolumn{1}{c}{X} & \multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{x_i-\bar x} & \multicolumn{1}{c}{(x_i-\bar x)^3 n_i} \\
% \hline
% (150,160] & 155 & 2 & -19.67 & -15221.00\\
% (160,170] & 165 & 8 & -9.67 & -7233.85\\
% (170,180] & 175 & 11 & 0.33 & 0.40\\
% (180,190] & 185 & 7 & 10.33 & 7716.12\\
% (190,200] & 195 & 2 & 20.33 & 16805.14\\ 
% \hline  
% \sum &  & 30 & & 2066.81 \\
% \hline
% \end{array}
% \end{center}
% \[
% g_1 = \frac{\sum (x_i-\bar x)^3n_i/n}{s^3} = \frac{2066.81/30}{10.1^3} = 0.07.
% \]
% Al estar tan próximo a 0, este valor indica que la distribución es prácticamente simétrica con respecto a la media. 
% 
% \note{Para calcular el coeficiente de asimetría en el ejemplo de las estaturas se puede añadir una nueva columna a la
% tabla de frecuencias con las desviaciones de los valores a la media que recordemos valía $174.67$ cm. Como habíamos agrupado los datos en
% clases, para calcular las desviaciones a la media se toma la marca de cada clase. Así, la primera desviación es 155 menos la media 174.67 lo
% que nos da $-19.67$ cm, la segunda es 165 menos $174.67$ cm y así sucesivamente. Obsérvese que las desviaciones de los valores menos que la
% media serán negativas y que las de los valores mayores serán positivas. A continuación se añade otra columna a la tabla con el producto de
% las desviaciones elevadas al cubo por su frecuencia absoluta, es decir, $-19.67$ al cubo por su frecuencia absoluta que es 2, lo que nos da
% $-15221$, $-9.67$ elevado al cubo y por su frecuencia absoluta que es 8, lo que nos da $-7233.85$, y así sucesivamente. Al final se suman
% los valores de esta columna y se dividen por el tamaño de la muestra que era 30. Por último el resultado de este cociente se vuelve a dividir por la desviación típica
% que era $10.1$ cm elevada al cubo, y se obtiene $0.07$.
% 
% Como este valor está muy próximo a 0, se puede concluir que la distribución de las estaturas es prácticamente simétrica.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de apuntamiento o curtosis}
% \begin{definition}[Coeficiente de apuntamiento muestral $g_2$]
% El \emph{coeficiente de apuntamiento muestral} de una variable $X$ se define como el promedio de las desviaciones de
% los valores de la muestra respecto de la media muestral, elevadas a la cuarta, dividido por la desviación típica a la
% cuarta y al resultado se le resta 3. \[g_2 = \frac{\sum (x_i-\bar x)^4 n_i/n}{s^4}-3 = \frac{\sum (x_i-\bar x)^4 f_i}{s^4}-3\]
% \end{definition}
% El coeficiente de apuntamiento muestral mide el grado de apuntamiento de los valores de la muestra con respecto a una distribución normal de referencia, de manera que:
% \begin{itemize}
% \item $g_2=0$ indica que la distribución tienen un apuntamiento normal (\emph{mesocúrtica}).
% \item $g_2<0$ indica que la distribución tiene menos apuntamiento de lo normal (\emph{platicúrtica}).
% \item $g_2>0$ indica que la distribución tiene más apuntamiento de lo normal (\emph{leptocúrtica}).
% \end{itemize}
% 
% \note{El apuntamiento de una distribución muestral tiene que ver con la pendiente su polígono de frecuencias.
% 
% El coeficiente de apuntamiento o kurtosis muestral, que se representa $g_2$ se define, como la suma del producto de las desviaciones de los
% valores de la muestra a la media muestral elevadas a la cuarta por su frecuencia absoluta, dividida por el tamaño de la muestra, y a su vez
% todo dividido por la desviación típica a la cuarta, y al final se resta 3 al cociente. Como puede verse, la fórmula es muy parecida a la del
% coeficiente de asimetría, pero tomando las potencias cuartas en lugar de las potencias al cubo, y restando 3 al cociente. 
% 
% Al igual que para el coeficiente de asimetría, como las desviaciones elevadas a la cuarta tienen las unidades de la variable a la cuarta y
% la desviación típica elevada al cubo también tiene las unidades de la variable a la cuarta, al realizar el cociente las unidades se cancelan
% y por tanto el coeficiente de apuntamiento es una medida adimensional que mide el grado de apuntamiento de la distribución muestral.
% 
% El apuntamiento suele medirse en comparación con un apuntamiento de referencia que es el de una distribución normal. La distribución normal
% se verá maś adelante en el curso, pero baste decir que es la distribución más común que se presenta en la naturaleza, y por lo tanto, está
% justificado tomarla como referencia y comparar el apuntamiento de cualquier otra distribución con el de la distribución normal que siempre
% vale 0. Por tanto cuando
% \begin{itemize}
% \item $g_2=0$ indica que la distribución tienen un apuntamiento normal (\emph{mesocúrtica}).
% \item $g_2<0$ indica que la distribución tiene menos apuntamiento de lo normal (\emph{platicúrtica}).
% \item $g_2>0$ indica que la distribución tiene más apuntamiento de lo normal (\emph{leptocúrtica}
% \end{itemize}
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de apuntamiento o curtosis}
% \framesubtitle{Ejemplo de distribución mesocúrtica}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_mesocurtica}}
% \end{center}
% \note{Aquí un histograma con coeficente de apuntamiento 0 y sobre él una distribución normal, representada por esta curva conocida
% como campana de Gauss. Obsérvese cómo la altura de las barras coinciden con la campaña de Gauss y se ajustan perfectamente a la
% distribución normal, lo que indica que la distribución es mesocúrtica.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de apuntamiento o curtosis}
% \framesubtitle{Ejemplo de distribución platicúrtica}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_platicurtica}}
% \end{center}
% \note{Ahora tenemos un histograma con un coeficiente de apuntamiento menor que 0. Como se puede apreciar, en este caso la altura de las
% barras centrales están por debajo de la campa de Gauss y la distribución tiene menos apuntamiento de lo normal, por lo que se dice que es
% platicúrtica.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Coeficiente de apuntamiento o curtosis}
% \framesubtitle{Ejemplo de distribución leptocúrtica}
% \begin{center}
% \scalebox{0.8}{\input{img/descriptive/distribucion_leptocurtica}}
% \end{center}
% \note{En este otro caso tenemos un histograma con un coeficiente de apuntamiento mayor que 0. Ahora la altura
% de las barras centrales están por encima de la campa de Gauss y la distribución tiene más apuntamiento de lo normal, por lo que se dice
% que es leptocúrtica.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Cálculo del coeficiente de apuntamiento}
% \framesubtitle{Ejemplo con datos agrupados}
% De nuevo para el ejemplo de las estaturas podemos calcular el coeficiente de asimetría a partir de la tabla de frecuencias añadiendo una
% nueva columna con las desviaciones a la media $\bar x = 174.67$ cm elevadas a la cuarta: 
% \begin{center}
% \setlength\arraycolsep{3mm}
% \setlength\arrayrulewidth{0.5pt}
% \begin{array}{rrrrr}
% \hline
% \multicolumn{1}{c}{X} & \multicolumn{1}{c}{x_i} & \multicolumn{1}{c}{n_i} & \multicolumn{1}{c}{x_i-\bar x} & \multicolumn{1}{c}{(x_i-\bar x)^4 n_i} \\
% \hline
% (150,160] & 155 & 2 & -19.67 & 299396.99\\
% (160,170] & 165 & 8 & -9.67 & 69951.31\\
% (170,180] & 175 & 11 & 0.33 & 0.13\\
% (180,190] & 185 & 7 & 10.33 & 79707.53\\
% (190,200] & 195 & 2 & 20.33 & 341648.49\\ 
% \hline 
% \sum &  & 30 & & 790704.45 \\
% \hline
% \end{array}
% \end{center}
% \[
% g_2 = \frac{\sum (x_i-\bar x)^4n_i/n}{s^4} - 3 = \frac{790704.45/30}{10.1^4}-3 = -0.47.
% \]
% Como se trata de un valor negativo, aunque pequeño, podemos decir que la distribución es ligeramente platicúrtica. 
% 
% \note{El coeficiente de apuntamiento se calcula de manera similar al coeficiente de asimetría, calculando primero las desviaciones a la
% meida en una columna de la tabla y luego añadiendo otra columna con el producto de
% las desviaciones elevadas a la cuarta por su frecuencia absoluta, es decir, $-19.67$ a la cuarta por su frecuencia absoluta que es 2, lo que
% nos da $299396.99$, $-9.67$ elevado a la cuarta y por su frecuencia absoluta que es 8, lo que nos da $69951.31$, y así sucesivamente. Al
% final se suman los valores de esta columna y se dividen por el tamaño de la muestra que era 30. Por último el resultado de este cociente se
% vuelve a dividir por la desviación típica que era $10.1$ cm elevada a la cuarta, y al resultado se le resta 3, obteniendo -0.47.
% 
% Como se trata de un valor negativo, aunque próximo a cero, se puede concluir que la distribución de las estaturas es ligeramente
% platicúrtica. }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Interpretación de los coeficientes de asimetría y apuntamiento}
% Como se verá más adelante en la parte de inferencia, muchas de las pruebas estadísticas solo pueden aplicarse a poblaciones normales.
% 
% Las poblaciones normales se caracterizan por ser simétricas y mesocúrticas, de manera que, tanto el coeficiente de asimetría como el de apuntamiento pueden utilizarse para contrastar si los datos de la muestra provienen de una población normal.
% 
% En general, se suele rechazar la hipótesis de normalidad de la población cuando $g_1$ o $g_2$ estén fuera del intervalo $[-2,2]$.
% 
% En tal caso, lo habitual es aplicar alguna transformación a la variable para corregir la anormalidad.
% 
% \note{Como se verá más adelante en la parte de inferencia, muchas de las pruebas estadísticas solo pueden aplicarse a poblaciones normales,
% que se caracterizan por ser simétricas y mesocúrticas, de manera que, tanto el coeficiente de asimetría como el de apuntamiento pueden
% utilizarse para comprobar si los datos de la muestra provienen de una población normal. 
% 
% En general, se suele rechazar la hipótesis de normalidad de la población cuando $g_1$ o $g_2$ estén fuera del intervalo $[-2,2]$.
% 
% En tal caso, lo habitual es aplicar alguna transformación a la variable para corregir la anormalidad.
% }
% \end{frame}
% 
% 
% \subsection{Transformaciones de variables}
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformaciones de variables}
% En muchas ocasiones se suelen transformar los datos brutos para trabajar con unas unidades más cómodas, o bien para corregir alguna
% anormalidad de la distribución.
% 
% 
% Por ejemplo, si estamos trabajando con estaturas medidas en metros y tenemos los siguientes valores:
% \[ 
% 1.75\mbox{m}, 1.65\mbox{m}, 1.80\mbox{m},
% \]
% podemos evitar los decimales multiplicando por 100, es decir, pasando de metros a centímetros:
% \[ 
% 175\mbox{cm}, 165\mbox{cm}, 180\mbox{cm},
% \]
% Y si queremos reducir la magnitud de los datos podemos restarles a todos el menor de ellos, en este caso, 165cm:
% \[ 
% 10\mbox{cm}, 0\mbox{cm}, 15\mbox{cm},
% \]
% Está claro que este conjunto de datos es mucho más sencillo que el original. En el fondo lo que se ha hecho es aplicar a los datos la
% transformación: \[Y= 100X-165\]
% 
% \note{En muchas ocasiones los datos brutos de la muestra suelen transformarse, a veces simplemente para cambiar a una escala más cómoda y
% otras veces para corregir alguna anormalidad de la distribución.
% 
% Si por ejemplo estamos trabajando con estaturas medidas en metros, con dos decimales como los de este ejemplo, podemos evitar el trabajo
% con decimales multiplicando por 100, es decir, pasando de metros a centímetros. Así tenemos que $1.75$ m multiplicado por 100 se transforma
% en 175 cm, $1.65$ m multiplicado por 100 se trasnforma en 165 cm y $1.80$ m se transforma en 180 cm.
% 
% Después, si queremos reducir la magnitud de los datos y pasar de centenas a unidades más pequeñas, podemos restarle a todos los datos el
% mínimo de los valores que es 165 cm. 175 cm menos 165 cm nos da 10 cm, 165 menos 165 nos da 0 cm y 180 menos 165 nos da 15 cm.
% 
% Con esto los datos pasan a una escala mucho más fácil de manejar que la original. En el fondo lo que hemos hecho es aplicar a cada dato la
% transformación lineal $Y=100x-165$.}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformaciones lineales}
% Una de las transformaciones más habituales es la \emph{transformación lineal}:
% \[
% Y=a+bX.
% \]
% Se puede comprobar fácilmente que la media y la desviación típica de la variable resultante cumplen:
% \begin{align*}
% \bar y &= a+ b\bar x,\\
% s_{y} &= |b|s_{x}
% \end{align*}
% Además, el coeficiente de curtosis no se altera y el de asimetría sólo cambia de signo si $b$ es negativo.
% 
% \note{Una de las transformaciones más habituales que suele realizarse es la \emph{transformación lineal} que sigue la ecuación de una recta
% $Y=a+bX$, donde $a$ es el término independiente y $b$ la pendiente de la recta.
% 
% Una propiedad que tiene esta transformación y resulta fácil de comprobar es que la media de la variable transformada se puede obtener
% aplicando la misma transformación lineal a la media de la variable original, es decir, $\bar y = a+ b\bar x$, y por otro lado, la desviación
% típica de la variable transformada se puede obtener multiplicando la desviación típica de la variable original por el valor absoluto de la
% pendiente de la trasnformación lineal, es decir,  $s_{y} &= |b|s_{x}$.
% 
% Además, el coeficiente de curtosis no se altera y el de asimetría sólo cambia de signo si la pendiente es negativa.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformación de tipificación y puntuaciones típicas}
% Una de las transformaciones lineales más habituales es la \emph{tipificación}:
% \begin{definition}[Variable tipificada]
% La \emph{variable tipificada} de una variable estadística $X$ es la variable que resulta de restarle su media y dividir por su desviación típica.
% \[
% Z=\frac{X-\bar x}{s_{x}}
% \]
% \end{definition}
% 
% La tipificación es muy útil para eliminar la dependencia de una variable respecto de las unidades de medida empleadas.
% 
% Los valores tipificados se conocen como \structure{\textbf{puntuaciones típicas}} y miden el número de desviaciones típicas que dista de la media cada observación, lo cual es útil para comparar variables con distintas unidades. 
% 
% Otra propiedad de la variable tipificada es que tiene media 0 y desviación típica 1:
% \[
% \bar z = 0 \qquad s_{z} = 1
% \]
% 
% \note{Entre las transformaciones lineales hay una de especial importancia, y se conoce como transformación de tipificación. La tipificación
% consiste en dividir las desviaciones de los valores a la media por la desviación típica. 
% 
% Como las desviaciones a la media tienen las unidades de la variable y la desviación típica también, al hacer el cociente se cancelan las
% unidades y los valores de la variable tipificada no tienen unidades, por lo que esta transformación es útil para eliminar la dependencia de
% la variable de las unidades de medida empleadas.
% 
% Los valores tipificados se conocen como \structure{\textbf{puntuaciones típicas}} y miden el número de desviaciones típicas que dista de la
% media cada observación, lo cual es útil para comparar variables con distintas unidades.
% 
% Otra propiedad que se deduce de las propiedades de las trasnformaciones lineales vistas antes es que la media de una variable tipificada
% siempre vale 0 y su desviación típica 1.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformación de tipificación y puntuaciones típicas}
% \framesubtitle{Ejemplo}
% Las notas de 5 alumnos en dos asignaturas $X$ e $Y$ son:
% \[
% \begin{array}{rccccccccc}
% \mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\ \cline{1-6}
% X: & 2 & 5 & 4 & \alert{8} & 6 & \qquad & \bar x = 5 & \quad s_x = 2\\
% Y: & 1 & 9 & \alert{8} & 5 & 2 & \qquad & \bar y = 5 & \quad s_y = 3.16\\
% \end{array}
% \]
% \begin{center}
% \emph{¿Han tenido el mismo rendimiento los alumnos que han sacado un 8?}
% \end{center}
% Podría parecer que ambos alumnos han tenido el mismo rendimiento puesto que tienen la misma nota, pero si queremos ver el rendimiento relativo al resto del grupo, tendríamos que tener en cuenta la dispersión de cada muestra y medir sus puntuaciones típicas:
% \[
% \begin{array}{cccccc}
% X: & -1.5 & 0 & -0.5 & \alert{1.5} & 0.5 \\
% Y: & -1.26 & 1.26 & \alert{0.95} & 0 & -0.95\\
% \end{array}
% \]
% Es decir, el alumno que tiene un 8 en $X$ está $1.5$ veces la desviación típica por encima de la media de su grupo, mientras que el alumno que tiene un 8 en $Y$ sólo está $0.95$ desviaciones típicas por encima de su media.
% Así pues, el primer alumno tuvo un rendimiento superior al segundo. 
% 
% \note{Para ver la utilidad de la transformación de tipificación, supongamos que tenemos un grupo de 5 alumnos en los que se ha medido la
% nota en dos asignaturas $X$ e $Y$. Si calculamos la media y la desviación típica en cada asignatura, se tiene que la nota media en $X$ es
% 5 con una desviación típica de $2$ y que la nota media de $Y$ es también 5 con una desviación típica de $3.16$, es decir, hay más dispersión
% en las notas de $Y$ que en las de $X$.
% 
% Podríamos preguntarnos si sacar un 8 en la asignatura $X$ tiene el mismo mérito que sacar un $8$ en la asignatura $Y$, o dicho de otro modo,
% el alumno que ha sacado un 8 en la asignatura $X$, ¿ha tenido el mismo rendimiento que el que ha sacado un 8 en la $Y$?
% 
% Podría parecer que ambos alumnos han tenido el mismo rendimiento puesto que tienen la misma nota, pero si queremos ver el rendimiento
% relativo al resto del grupo, tendríamos que tener en cuenta la dispersión de cada muestra y medir sus puntuaciones típicas, que son 
% \[
% \begin{array}{cccccc}
% X: & -1.5 & 0 & -0.5 & \alert{1.5} & 0.5 \\
% Y: & -1.26 & 1.26 & \alert{0.95} & 0 & -0.95\\
% \end{array}
% \]
% Es decir, el alumno que tiene un 8 en $X$ está $1.5$ veces la desviación típica por encima de la media de su grupo, mientras que el alumno
% que tiene un 8 en $Y$ sólo está $0.95$ desviaciones típicas por encima de su media. Así pues, el primer alumno tuvo un rendimiento superior
% al segundo y tiene más mérito sacar un $8$ en la asignatura $X$ que en la $Y$.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformación de tipificación y puntuaciones típicas}
% \framesubtitle{Ejemplo}
% Siguiendo con el ejemplo anterior
% \begin{center}
% \emph{¿Cuál es el mejor alumno?}
% \end{center}
% Si simplemente se suman las puntuaciones de cada asignatura se tiene:
% \[
% \begin{array}{rccccc}
% \mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\ \hline
% X: & 2 & 5 & 4 & 8 & 6 \\
% Y: & 1 & 9 & 8 & 5 & 2 \\ \hline
% \sum & 3 & \alert{14} & 12 & 13 & 8 
% \end{array}
% \]
% El mejor alumno sería el segundo.
% 
% Pero si se considera el rendimiento relativo tomando las puntuaciones típicas se tiene:
% \[
% \begin{array}{rccccc}
% \mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\ \hline
% X: & -1.5 & 0 & -0.5 & 1.5 & 0.5 \\
% Y: & -1.26 & 1.26 & 0.95 & 0 & -0.95\\ \hline
% \sum & -2.76 & 1.26 & 0.45 & \alert{1.5} & -0.45
% \end{array}
% \]
% Y el mejor alumno sería el cuarto. 
% 
% \note{Siguiendo con el ejemplo anterior, también podríamos habernos preguntado ¿cuál es el mejor alumno?  
% Si simplemente sumamos las puntuaciones de cada asignatura tenemos:
% \[
% \begin{array}{rccccc}
% \mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\ \hline
% X: & 2 & 5 & 4 & 8 & 6 \\
% Y: & 1 & 9 & 8 & 5 & 2 \\ \hline
% \sum & 3 & \alert{14} & 12 & 13 & 8 
% \end{array}
% \]
% El mejor alumno sería el segundo.
% 
% Pero es mucho más razonable cosiderar el rendimiento relativo tomando las puntuaciones típicas y entonces se tiene que la suma de las
% puntuaciones típicas es:
% \[
% \begin{array}{rccccc}
% \mbox{Alumno:} & 1 & 2 & 3 & 4 & 5\\ \hline
% X: & -1.5 & 0 & -0.5 & 1.5 & 0.5 \\
% Y: & -1.26 & 1.26 & 0.95 & 0 & -0.95\\ \hline
% \sum & -2.76 & 1.26 & 0.45 & \alert{1.5} & -0.45
% \end{array}
% \]
% Con lo que realmente el mejor alumno es el cuarto. 
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformaciones no lineales}
% La transformación $Y=X^2$ comprime la escala para valores pequeños y la expande para valores altos, de manera que es muy útil para corregir asimetrías hacia la izquierda.
% \begin{center}
% \scalebox{1}{
% \begin{pspicture}(0,0)(12,6)
% \rput[l](0,3){\includegraphics[scale=0.4]{img/descriptive/histograma_asimetrico_izquierda.eps}}
% \psline[linewidth=15pt,linecolor=royalblue1,arrowlength=1,arrowinset=0]{->}(5,3)(7,3)
% \rput(6,3){$Y=X^2$}
% \rput[l](7,3){\includegraphics[scale=0.4]{img/descriptive/histograma_simetrico.eps}}
% \end{pspicture}}
% \end{center}
% 
% \note{Otras transformaciones no lineales que son habituales para corregir anormalidades de la muestra son el cuadrado, que comprime la
% escala para valores pequeños y la expande para valores altos, de manera que es muy útil para corregir asimetrías hacia la izquierda, tal y
% como puede apreciarse en estos histogramas.
% }
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Transformaciones no lineales}
% Las transformaciones $Y=\sqrt x$, $Y= \log X$ y $Y=1/X$ comprimen la escala para valores altos y la expanden para valores pequeños, de manera que son útiles para corregir asimetrías hacia la
% derecha.
% \begin{center}
% \scalebox{1}{
% \begin{pspicture}(0,0)(12,6)
% \rput[l](0,3){\reflectbox{\includegraphics[scale=0.4]{img/descriptive/histograma_asimetrico_izquierda.eps}}}
% \psline[linewidth=15pt,linecolor=royalblue1,arrowlength=1,arrowinset=0]{->}(5,3)(7,3)
% \rput(6,3){$Y=\sqrt{X}$}
% \rput[l](7,3){\includegraphics[scale=0.4]{img/descriptive/histograma_simetrico.eps}}
% \end{pspicture}}
% \end{center}
% 
% \note{Mientras que para corregir asimetrías hacia la derecha se utilizan o bien la raíz cuadrada, la función logarítmica o la inversa, ya
% que ambas comprimen la escala para valores altos y la expanden para valores pequeños.}
% \end{frame}
% 
% 


% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Factors}
% Sometimes is interesting to describe the frequency distribution of the main variable for different subsamples
% corresponding to the categories of another variable that is known as \structure{\textbf{classificatory variable}} or
% \structure{\textbf{factor}}.
% 
% \structure{Example} Dividing the sample of heights by sex we get two subsamples
% \begin{center}
% \begin{tabular}{lll}
% \hline
% \multirow{2}{*}{Women} &
% 173, 158, 174, 166, 162, 177, 165, 154, 166, 182, \\
% & 169, 172, 170, 168. \\
% \hline
% \multirow{2}{*}{Men} &
% 179, 181, 172, 194, 185, 187, 198, 178, 188, 171,\\
% & 175, 167, 186, 172, 176, 187. \\
% \hline
% \end{tabular}
% \end{center}
% \end{frame}
% 
% 
% %---------------------------------------------------------------------slide----
% \begin{frame}
% \frametitle{Comparing distributions for the levels of a factor }
% 
% \begin{center}
% \scalebox{0.5\textwidth}{\input{img/descriptive/factor_histogram}}
% \quad
% \scalebox{0.5\textwidth}{\input{img/descriptive/factor_box_plot}}
% \end{center}
% \end{frame}
